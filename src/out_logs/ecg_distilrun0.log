ot.gpu not found - coupling computation will be in cpu
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode
config:  ./configs/ecg.yaml
args.device:  cuda
NVIDIA GeForce RTX 3090
target_seq_len 64
/home/aryas/ORCA/src
./datasets
src feat shape torch.Size([2000, 768]) torch.Size([2000]) num classes 4
class weights
0 79086 296044 0.2671427220278067
1 72521 296044 0.2449669643701612
2 81419 296044 0.2750233073462053
3 63018 296044 0.21286700625582683
Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 480/480 [00:00<00:00, 190kB/s]
Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]Downloading model.safetensors:   3%|▎         | 10.5M/331M [00:00<00:04, 64.1MB/s]Downloading model.safetensors:  10%|▉         | 31.5M/331M [00:00<00:03, 97.4MB/s]Downloading model.safetensors:  16%|█▌        | 52.4M/331M [00:00<00:02, 107MB/s] Downloading model.safetensors:  22%|██▏       | 73.4M/331M [00:00<00:02, 111MB/s]Downloading model.safetensors:  29%|██▊       | 94.4M/331M [00:00<00:02, 113MB/s]Downloading model.safetensors:  35%|███▍      | 115M/331M [00:01<00:01, 115MB/s] Downloading model.safetensors:  41%|████      | 136M/331M [00:01<00:01, 116MB/s]Downloading model.safetensors:  48%|████▊     | 157M/331M [00:01<00:01, 116MB/s]Downloading model.safetensors:  54%|█████▍    | 178M/331M [00:01<00:01, 117MB/s]Downloading model.safetensors:  60%|██████    | 199M/331M [00:01<00:01, 117MB/s]Downloading model.safetensors:  67%|██████▋   | 220M/331M [00:01<00:00, 117MB/s]Downloading model.safetensors:  73%|███████▎  | 241M/331M [00:02<00:00, 117MB/s]Downloading model.safetensors:  79%|███████▉  | 262M/331M [00:02<00:00, 117MB/s]Downloading model.safetensors:  86%|████████▌ | 283M/331M [00:02<00:00, 117MB/s]Downloading model.safetensors:  92%|█████████▏| 304M/331M [00:02<00:00, 117MB/s]Downloading model.safetensors:  98%|█████████▊| 325M/331M [00:02<00:00, 117MB/s]Downloading model.safetensors: 100%|██████████| 331M/331M [00:02<00:00, 114MB/s]
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Params to learn: 	embedder.norm.weight	embedder.norm.bias	embedder.position_embeddings.weight	embedder.projection.weight	embedder.projection.bias	predictor.weight	predictor.bias
[train embedder 0 0.000010 ] time elapsed: 6.9174 	otdd loss: 24.1039
[train embedder 1 0.000010 ] time elapsed: 1.6842 	otdd loss: 23.6709
[train embedder 2 0.000010 ] time elapsed: 1.7128 	otdd loss: 23.5926
[train embedder 3 0.000010 ] time elapsed: 1.6845 	otdd loss: 23.1349
[train embedder 4 0.000010 ] time elapsed: 1.7009 	otdd loss: 22.1196
[train embedder 5 0.000010 ] time elapsed: 1.7163 	otdd loss: 22.0244
[train embedder 6 0.000010 ] time elapsed: 1.7434 	otdd loss: 21.1773
[train embedder 7 0.000010 ] time elapsed: 1.7109 	otdd loss: 20.1129
[train embedder 8 0.000010 ] time elapsed: 1.7045 	otdd loss: 18.6002
[train embedder 9 0.000010 ] time elapsed: 1.7231 	otdd loss: 18.3870
[train embedder 10 0.000010 ] time elapsed: 1.7089 	otdd loss: 18.3646
[train embedder 11 0.000010 ] time elapsed: 1.7378 	otdd loss: 16.5268
[train embedder 12 0.000010 ] time elapsed: 1.8443 	otdd loss: 15.5710
[train embedder 13 0.000010 ] time elapsed: 1.7005 	otdd loss: 14.5954
[train embedder 14 0.000010 ] time elapsed: 1.7276 	otdd loss: 13.9920
[train embedder 15 0.000010 ] time elapsed: 1.7137 	otdd loss: 13.2501
[train embedder 16 0.000010 ] time elapsed: 1.6954 	otdd loss: 12.1013
[train embedder 17 0.000010 ] time elapsed: 1.7106 	otdd loss: 11.7470
[train embedder 18 0.000010 ] time elapsed: 1.7092 	otdd loss: 11.4639
[train embedder 19 0.000010 ] time elapsed: 1.7516 	otdd loss: 10.5229
[train embedder 20 0.000010 ] time elapsed: 1.7223 	otdd loss: 10.4901
[train embedder 21 0.000002 ] time elapsed: 1.6914 	otdd loss: 9.7884
[train embedder 22 0.000002 ] time elapsed: 1.7105 	otdd loss: 9.8560
[train embedder 23 0.000002 ] time elapsed: 1.7044 	otdd loss: 9.5485
[train embedder 24 0.000002 ] time elapsed: 1.6956 	otdd loss: 9.1866
[train embedder 25 0.000002 ] time elapsed: 1.7132 	otdd loss: 9.6439
[train embedder 26 0.000002 ] time elapsed: 1.7094 	otdd loss: 9.4432
[train embedder 27 0.000002 ] time elapsed: 1.6995 	otdd loss: 9.5237
[train embedder 28 0.000002 ] time elapsed: 1.6943 	otdd loss: 8.8077
[train embedder 29 0.000002 ] time elapsed: 1.6923 	otdd loss: 8.7198
[train embedder 30 0.000002 ] time elapsed: 1.7201 	otdd loss: 9.0647
[train embedder 31 0.000002 ] time elapsed: 1.7000 	otdd loss: 9.0712
[train embedder 32 0.000002 ] time elapsed: 1.7048 	otdd loss: 8.7592
[train embedder 33 0.000002 ] time elapsed: 1.6980 	otdd loss: 8.5347
[train embedder 34 0.000002 ] time elapsed: 1.8355 	otdd loss: 8.9501
[train embedder 35 0.000002 ] time elapsed: 1.7080 	otdd loss: 8.2023
[train embedder 36 0.000002 ] time elapsed: 1.7098 	otdd loss: 8.2858
[train embedder 37 0.000002 ] time elapsed: 1.7544 	otdd loss: 8.8663
[train embedder 38 0.000002 ] time elapsed: 1.7253 	otdd loss: 8.0825
[train embedder 39 0.000002 ] time elapsed: 1.7525 	otdd loss: 8.3836
[train embedder 40 0.000002 ] time elapsed: 1.7241 	otdd loss: 8.2920
[train embedder 41 0.000000 ] time elapsed: 1.7518 	otdd loss: 8.7678
[train embedder 42 0.000000 ] time elapsed: 1.7285 	otdd loss: 7.9189
[train embedder 43 0.000000 ] time elapsed: 1.7106 	otdd loss: 7.8224
[train embedder 44 0.000000 ] time elapsed: 1.6987 	otdd loss: 8.3093
[train embedder 45 0.000000 ] time elapsed: 1.7112 	otdd loss: 8.1117
[train embedder 46 0.000000 ] time elapsed: 1.7280 	otdd loss: 8.3640
[train embedder 47 0.000000 ] time elapsed: 1.7202 	otdd loss: 7.8590
[train embedder 48 0.000000 ] time elapsed: 1.7238 	otdd loss: 7.7415
[train embedder 49 0.000000 ] time elapsed: 1.7762 	otdd loss: 8.1155
[train embedder 50 0.000000 ] time elapsed: 1.7062 	otdd loss: 7.9729
[train embedder 51 0.000000 ] time elapsed: 1.7428 	otdd loss: 7.8015
[train embedder 52 0.000000 ] time elapsed: 1.7143 	otdd loss: 7.6848
[train embedder 53 0.000000 ] time elapsed: 1.7130 	otdd loss: 8.2605
[train embedder 54 0.000000 ] time elapsed: 1.7123 	otdd loss: 8.0000
[train embedder 55 0.000000 ] time elapsed: 1.7057 	otdd loss: 7.8125
[train embedder 56 0.000000 ] time elapsed: 1.8481 	otdd loss: 7.9956
[train embedder 57 0.000000 ] time elapsed: 1.7263 	otdd loss: 8.2187
[train embedder 58 0.000000 ] time elapsed: 1.7063 	otdd loss: 7.8370
[train embedder 59 0.000000 ] time elapsed: 1.7392 	otdd loss: 7.8860
embedder_stats_saved:  None
Params to learn: 	model.encoder.layer.0.attention.self.query.weight	model.encoder.layer.0.attention.self.query.bias	model.encoder.layer.0.attention.self.key.weight	model.encoder.layer.0.attention.self.key.bias	model.encoder.layer.0.attention.self.value.weight	model.encoder.layer.0.attention.self.value.bias	model.encoder.layer.0.attention.output.dense.weight	model.encoder.layer.0.attention.output.dense.bias	model.encoder.layer.0.attention.output.LayerNorm.weight	model.encoder.layer.0.attention.output.LayerNorm.bias	model.encoder.layer.0.intermediate.dense.weight	model.encoder.layer.0.intermediate.dense.bias	model.encoder.layer.0.output.dense.weight	model.encoder.layer.0.output.dense.bias	model.encoder.layer.0.output.LayerNorm.weight	model.encoder.layer.0.output.LayerNorm.bias	model.encoder.layer.1.attention.self.query.weight	model.encoder.layer.1.attention.self.query.bias	model.encoder.layer.1.attention.self.key.weight	model.encoder.layer.1.attention.self.key.bias	model.encoder.layer.1.attention.self.value.weight	model.encoder.layer.1.attention.self.value.bias	model.encoder.layer.1.attention.output.dense.weight	model.encoder.layer.1.attention.output.dense.bias	model.encoder.layer.1.attention.output.LayerNorm.weight	model.encoder.layer.1.attention.output.LayerNorm.bias	model.encoder.layer.1.intermediate.dense.weight	model.encoder.layer.1.intermediate.dense.bias	model.encoder.layer.1.output.dense.weight	model.encoder.layer.1.output.dense.bias	model.encoder.layer.1.output.LayerNorm.weight	model.encoder.layer.1.output.LayerNorm.bias	model.encoder.layer.2.attention.self.query.weight	model.encoder.layer.2.attention.self.query.bias	model.encoder.layer.2.attention.self.key.weight	model.encoder.layer.2.attention.self.key.bias	model.encoder.layer.2.attention.self.value.weight	model.encoder.layer.2.attention.self.value.bias	model.encoder.layer.2.attention.output.dense.weight	model.encoder.layer.2.attention.output.dense.bias	model.encoder.layer.2.attention.output.LayerNorm.weight	model.encoder.layer.2.attention.output.LayerNorm.bias	model.encoder.layer.2.intermediate.dense.weight	model.encoder.layer.2.intermediate.dense.bias	model.encoder.layer.2.output.dense.weight	model.encoder.layer.2.output.dense.bias	model.encoder.layer.2.output.LayerNorm.weight	model.encoder.layer.2.output.LayerNorm.bias	model.encoder.layer.3.attention.self.query.weight	model.encoder.layer.3.attention.self.query.bias	model.encoder.layer.3.attention.self.key.weight	model.encoder.layer.3.attention.self.key.bias	model.encoder.layer.3.attention.self.value.weight	model.encoder.layer.3.attention.self.value.bias	model.encoder.layer.3.attention.output.dense.weight	model.encoder.layer.3.attention.output.dense.bias	model.encoder.layer.3.attention.output.LayerNorm.weight	model.encoder.layer.3.attention.output.LayerNorm.bias	model.encoder.layer.3.intermediate.dense.weight	model.encoder.layer.3.intermediate.dense.bias	model.encoder.layer.3.output.dense.weight	model.encoder.layer.3.output.dense.bias	model.encoder.layer.3.output.LayerNorm.weight	model.encoder.layer.3.output.LayerNorm.bias	model.encoder.layer.4.attention.self.query.weight	model.encoder.layer.4.attention.self.query.bias	model.encoder.layer.4.attention.self.key.weight	model.encoder.layer.4.attention.self.key.bias	model.encoder.layer.4.attention.self.value.weight	model.encoder.layer.4.attention.self.value.bias	model.encoder.layer.4.attention.output.dense.weight	model.encoder.layer.4.attention.output.dense.bias	model.encoder.layer.4.attention.output.LayerNorm.weight	model.encoder.layer.4.attention.output.LayerNorm.bias	model.encoder.layer.4.intermediate.dense.weight	model.encoder.layer.4.intermediate.dense.bias	model.encoder.layer.4.output.dense.weight	model.encoder.layer.4.output.dense.bias	model.encoder.layer.4.output.LayerNorm.weight	model.encoder.layer.4.output.LayerNorm.bias	model.encoder.layer.5.attention.self.query.weight	model.encoder.layer.5.attention.self.query.bias	model.encoder.layer.5.attention.self.key.weight	model.encoder.layer.5.attention.self.key.bias	model.encoder.layer.5.attention.self.value.weight	model.encoder.layer.5.attention.self.value.bias	model.encoder.layer.5.attention.output.dense.weight	model.encoder.layer.5.attention.output.dense.bias	model.encoder.layer.5.attention.output.LayerNorm.weight	model.encoder.layer.5.attention.output.LayerNorm.bias	model.encoder.layer.5.intermediate.dense.weight	model.encoder.layer.5.intermediate.dense.bias	model.encoder.layer.5.output.dense.weight	model.encoder.layer.5.output.dense.bias	model.encoder.layer.5.output.LayerNorm.weight	model.encoder.layer.5.output.LayerNorm.bias	embedder.norm.weight	embedder.norm.bias	embedder.position_embeddings.weight	embedder.projection.weight	embedder.projection.bias	predictor.weight	predictor.bias
using cuda

------- Experiment Summary --------
id: 0
dataset: ECG 	batch size: 4 	lr: 1e-06
num train batch: 74011 	num validation batch: 8118 	num test batch: 8118
finetune method: all
param count: 42942724 42942724
wrapper1D(
  (model): RobertaModel(
    (embeddings): embedder_placeholder()
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-5): 6 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (pooler): adaptive_pooler(
      (pooler): AdaptiveAvgPool1d(output_size=1)
    )
  )
  (embedder): Embeddings1D(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (projection): Conv1d(1, 768, kernel_size=(20,), stride=(20,))
  )
  (predictor): Linear(in_features=768, out_features=4, bias=True)
)

------- Start Training --------
epoch:  0
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 0 0.000000 ] time elapsed: 1567.9190 	train loss: 1.2422 	val loss: 1.0992 	val score: 0.5117 	best val score: 0.5117
epoch:  1
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 1 0.000000 ] time elapsed: 1562.3926 	train loss: 1.0080 	val loss: 1.0326 	val score: 0.4627 	best val score: 0.4627
epoch:  2
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 2 0.000001 ] time elapsed: 1566.7460 	train loss: 0.9440 	val loss: 0.9690 	val score: 0.4385 	best val score: 0.4385
epoch:  3
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 3 0.000001 ] time elapsed: 1583.6909 	train loss: 0.8946 	val loss: 0.9507 	val score: 0.4143 	best val score: 0.4143
epoch:  4
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 4 0.000001 ] time elapsed: 1575.9993 	train loss: 0.8458 	val loss: 0.9122 	val score: 0.3893 	best val score: 0.3893
epoch:  5
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 5 0.000001 ] time elapsed: 1569.7969 	train loss: 0.7942 	val loss: 0.8950 	val score: 0.3988 	best val score: 0.3893
epoch:  6
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 6 0.000001 ] time elapsed: 1575.6091 	train loss: 0.7529 	val loss: 0.8568 	val score: 0.3661 	best val score: 0.3661
epoch:  7
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 7 0.000001 ] time elapsed: 1586.7118 	train loss: 0.7261 	val loss: 0.8130 	val score: 0.3413 	best val score: 0.3413
epoch:  8
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 8 0.000001 ] time elapsed: 1577.4311 	train loss: 0.7045 	val loss: 0.8377 	val score: 0.3558 	best val score: 0.3413
epoch:  9
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 9 0.000001 ] time elapsed: 1581.0030 	train loss: 0.6876 	val loss: 0.7952 	val score: 0.3361 	best val score: 0.3361
epoch:  10
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 10 0.000001 ] time elapsed: 1588.5900 	train loss: 0.6824 	val loss: 0.8063 	val score: 0.3299 	best val score: 0.3299
epoch:  11
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 11 0.000001 ] time elapsed: 1593.9377 	train loss: 0.6790 	val loss: 0.8010 	val score: 0.3358 	best val score: 0.3299
epoch:  12
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[train full 12 0.000001 ] time elapsed: 1587.4789 	train loss: 0.6797 	val loss: 0.8183 	val score: 0.3332 	best val score: 0.3299
epoch:  13

ot.gpu not found - coupling computation will be in cpu
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode
config:  ./configs/cifar100.yaml
args.device:  cuda
NVIDIA GeForce RTX 3090
target_seq_len 512
Files already downloaded and verified
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz
  0%|          | 0/169001437 [00:00<?, ?it/s]  0%|          | 98304/169001437 [00:00<03:40, 764779.05it/s]  0%|          | 819200/169001437 [00:00<00:48, 3485899.58it/s]  2%|▏         | 4161536/169001437 [00:00<00:10, 15058795.63it/s]  6%|▋         | 10715136/169001437 [00:00<00:04, 32999967.19it/s] 10%|█         | 17235968/169001437 [00:00<00:03, 43835981.30it/s] 14%|█▍        | 23756800/169001437 [00:00<00:02, 50761371.02it/s] 18%|█▊        | 30277632/169001437 [00:00<00:02, 55317628.03it/s] 22%|██▏       | 36732928/169001437 [00:00<00:02, 58162606.03it/s] 26%|██▌       | 43319296/169001437 [00:00<00:02, 60473330.98it/s] 29%|██▉       | 49840128/169001437 [00:01<00:01, 61856634.83it/s] 33%|███▎      | 56328192/169001437 [00:01<00:01, 62707946.07it/s] 37%|███▋      | 62881792/169001437 [00:01<00:01, 63525338.50it/s] 41%|████      | 69402624/169001437 [00:01<00:01, 63988924.27it/s] 45%|████▍     | 75923456/169001437 [00:01<00:01, 64352410.20it/s] 49%|████▉     | 82411520/169001437 [00:01<00:01, 64453187.47it/s] 53%|█████▎    | 88899584/169001437 [00:01<00:01, 64525925.38it/s] 56%|█████▋    | 95387648/169001437 [00:01<00:01, 63938795.55it/s] 60%|██████    | 101875712/169001437 [00:01<00:01, 63609053.10it/s] 64%|██████▍   | 108265472/169001437 [00:01<00:00, 63215876.82it/s] 68%|██████▊   | 114753536/169001437 [00:02<00:00, 63221184.07it/s] 72%|███████▏  | 121110528/169001437 [00:02<00:00, 62871981.79it/s] 75%|███████▌  | 127500288/169001437 [00:02<00:00, 62924126.19it/s] 79%|███████▉  | 133988352/169001437 [00:02<00:00, 63472677.20it/s] 83%|████████▎ | 140509184/169001437 [00:02<00:00, 63966609.29it/s] 87%|████████▋ | 147062784/169001437 [00:02<00:00, 64402541.33it/s] 91%|█████████ | 153583616/169001437 [00:02<00:00, 64611135.34it/s] 95%|█████████▍| 160104448/169001437 [00:02<00:00, 64757972.30it/s] 99%|█████████▊| 166658048/169001437 [00:02<00:00, 64937819.39it/s]100%|██████████| 169001437/169001437 [00:02<00:00, 57711913.48it/s]
Extracting ./datasets/cifar-100-python.tar.gz to ./datasets
src feat shape torch.Size([5000, 128]) torch.Size([5000]) num classes 100
class weights
0 500 50000 0.01
1 500 50000 0.01
2 500 50000 0.01
3 500 50000 0.01
4 500 50000 0.01
5 500 50000 0.01
6 500 50000 0.01
7 500 50000 0.01
8 500 50000 0.01
9 500 50000 0.01
10 500 50000 0.01
11 500 50000 0.01
12 500 50000 0.01
13 500 50000 0.01
14 500 50000 0.01
15 500 50000 0.01
16 500 50000 0.01
17 500 50000 0.01
18 500 50000 0.01
19 500 50000 0.01
20 500 50000 0.01
21 500 50000 0.01
22 500 50000 0.01
23 500 50000 0.01
24 500 50000 0.01
25 500 50000 0.01
26 500 50000 0.01
27 500 50000 0.01
28 500 50000 0.01
29 500 50000 0.01
30 500 50000 0.01
31 500 50000 0.01
32 500 50000 0.01
33 500 50000 0.01
34 500 50000 0.01
35 500 50000 0.01
36 500 50000 0.01
37 500 50000 0.01
38 500 50000 0.01
39 500 50000 0.01
40 500 50000 0.01
41 500 50000 0.01
42 500 50000 0.01
43 500 50000 0.01
44 500 50000 0.01
45 500 50000 0.01
46 500 50000 0.01
47 500 50000 0.01
48 500 50000 0.01
49 500 50000 0.01
50 500 50000 0.01
51 500 50000 0.01
52 500 50000 0.01
53 500 50000 0.01
54 500 50000 0.01
55 500 50000 0.01
56 500 50000 0.01
57 500 50000 0.01
58 500 50000 0.01
59 500 50000 0.01
60 500 50000 0.01
61 500 50000 0.01
62 500 50000 0.01
63 500 50000 0.01
64 500 50000 0.01
65 500 50000 0.01
66 500 50000 0.01
67 500 50000 0.01
68 500 50000 0.01
69 500 50000 0.01
70 500 50000 0.01
71 500 50000 0.01
72 500 50000 0.01
73 500 50000 0.01
74 500 50000 0.01
75 500 50000 0.01
76 500 50000 0.01
77 500 50000 0.01
78 500 50000 0.01
79 500 50000 0.01
80 500 50000 0.01
81 500 50000 0.01
82 500 50000 0.01
83 500 50000 0.01
84 500 50000 0.01
85 500 50000 0.01
86 500 50000 0.01
87 500 50000 0.01
88 500 50000 0.01
89 500 50000 0.01
90 500 50000 0.01
91 500 50000 0.01
92 500 50000 0.01
93 500 50000 0.01
94 500 50000 0.01
95 500 50000 0.01
96 500 50000 0.01
97 500 50000 0.01
98 500 50000 0.01
99 500 50000 0.01
Params to learn: 	model.swin.embeddings.projection.weight	model.swin.embeddings.projection.bias	model.swin.embeddings.norm.weight	model.swin.embeddings.norm.bias	predictor.weight	predictor.bias
[train embedder 0 0.001000 ] time elapsed: 72.6301 	otdd loss: 4.5893
[train embedder 1 0.001000 ] time elapsed: 71.7612 	otdd loss: 4.5097
[train embedder 2 0.001000 ] time elapsed: 71.6096 	otdd loss: 4.5444
[train embedder 3 0.001000 ] time elapsed: 71.7735 	otdd loss: 5.0323
[train embedder 4 0.001000 ] time elapsed: 71.9136 	otdd loss: 5.6234
[train embedder 5 0.001000 ] time elapsed: 71.7769 	otdd loss: 6.1989
[train embedder 6 0.001000 ] time elapsed: 72.2116 	otdd loss: 5.7929
[train embedder 7 0.001000 ] time elapsed: 72.1424 	otdd loss: 4.9845
[train embedder 8 0.001000 ] time elapsed: 71.6012 	otdd loss: 4.9815
[train embedder 9 0.001000 ] time elapsed: 71.6711 	otdd loss: 5.2425
[train embedder 10 0.001000 ] time elapsed: 71.5352 	otdd loss: 4.8461
[train embedder 11 0.001000 ] time elapsed: 71.5875 	otdd loss: 4.6915
[train embedder 12 0.001000 ] time elapsed: 71.5337 	otdd loss: 4.8791
[train embedder 13 0.001000 ] time elapsed: 71.2813 	otdd loss: 4.7019
[train embedder 14 0.001000 ] time elapsed: 71.7184 	otdd loss: 4.5026
[train embedder 15 0.001000 ] time elapsed: 71.9824 	otdd loss: 4.6277
[train embedder 16 0.001000 ] time elapsed: 72.0372 	otdd loss: 4.7025
[train embedder 17 0.001000 ] time elapsed: 71.8053 	otdd loss: 4.5741
[train embedder 18 0.001000 ] time elapsed: 71.9002 	otdd loss: 4.3631
[train embedder 19 0.001000 ] time elapsed: 71.6780 	otdd loss: 4.3660
[train embedder 20 0.001000 ] time elapsed: 71.5359 	otdd loss: 4.3785
[train embedder 21 0.000200 ] time elapsed: 71.5603 	otdd loss: 4.5292
[train embedder 22 0.000200 ] time elapsed: 71.6001 	otdd loss: 4.3538
[train embedder 23 0.000200 ] time elapsed: 71.4784 	otdd loss: 4.2066
[train embedder 24 0.000200 ] time elapsed: 71.4657 	otdd loss: 4.3366
[train embedder 25 0.000200 ] time elapsed: 71.4544 	otdd loss: 4.3181
[train embedder 26 0.000200 ] time elapsed: 71.3585 	otdd loss: 4.1913
[train embedder 27 0.000200 ] time elapsed: 71.4845 	otdd loss: 4.2346
[train embedder 28 0.000200 ] time elapsed: 71.7017 	otdd loss: 4.3222
[train embedder 29 0.000200 ] time elapsed: 71.5622 	otdd loss: 4.1642
[train embedder 30 0.000200 ] time elapsed: 71.6207 	otdd loss: 4.1425
[train embedder 31 0.000200 ] time elapsed: 71.6536 	otdd loss: 4.1723
[train embedder 32 0.000200 ] time elapsed: 71.6103 	otdd loss: 4.1794
[train embedder 33 0.000200 ] time elapsed: 71.5773 	otdd loss: 4.1139
[train embedder 34 0.000200 ] time elapsed: 71.6504 	otdd loss: 4.1462
[train embedder 35 0.000200 ] time elapsed: 71.5568 	otdd loss: 4.1939
[train embedder 36 0.000200 ] time elapsed: 71.8191 	otdd loss: 4.0898
[train embedder 37 0.000200 ] time elapsed: 71.9407 	otdd loss: 4.0595
[train embedder 38 0.000200 ] time elapsed: 71.8639 	otdd loss: 4.1773
[train embedder 39 0.000200 ] time elapsed: 72.5517 	otdd loss: 4.1369
[train embedder 40 0.000200 ] time elapsed: 71.9157 	otdd loss: 4.1019
[train embedder 41 0.000040 ] time elapsed: 71.9896 	otdd loss: 4.1175
[train embedder 42 0.000040 ] time elapsed: 72.2055 	otdd loss: 4.1002
[train embedder 43 0.000040 ] time elapsed: 72.0600 	otdd loss: 4.1579
[train embedder 44 0.000040 ] time elapsed: 72.0876 	otdd loss: 4.1272
[train embedder 45 0.000040 ] time elapsed: 72.0512 	otdd loss: 4.1553
[train embedder 46 0.000040 ] time elapsed: 72.0877 	otdd loss: 4.0959
[train embedder 47 0.000040 ] time elapsed: 72.1572 	otdd loss: 4.0814
[train embedder 48 0.000040 ] time elapsed: 71.9973 	otdd loss: 4.0741
[train embedder 49 0.000040 ] time elapsed: 72.0244 	otdd loss: 4.1056
[train embedder 50 0.000040 ] time elapsed: 72.1012 	otdd loss: 4.0920
[train embedder 51 0.000040 ] time elapsed: 72.0089 	otdd loss: 4.1096
[train embedder 52 0.000040 ] time elapsed: 72.2857 	otdd loss: 4.1301
[train embedder 53 0.000040 ] time elapsed: 72.2867 	otdd loss: 4.0740
[train embedder 54 0.000040 ] time elapsed: 72.3199 	otdd loss: 4.0972
[train embedder 55 0.000040 ] time elapsed: 72.1607 	otdd loss: 4.1049
[train embedder 56 0.000040 ] time elapsed: 72.0583 	otdd loss: 4.1118
[train embedder 57 0.000040 ] time elapsed: 71.9162 	otdd loss: 4.0719
[train embedder 58 0.000040 ] time elapsed: 72.0959 	otdd loss: 4.0857
[train embedder 59 0.000040 ] time elapsed: 71.9133 	otdd loss: 4.1447
Files already downloaded and verified
embedder_stats_saved:  None
Params to learn: 	model.swin.embeddings.projection.weight	model.swin.embeddings.projection.bias	model.swin.embeddings.norm.weight	model.swin.embeddings.norm.bias	model.swin.encoder.layers.0.blocks.0.layernorm_before.weight	model.swin.encoder.layers.0.blocks.0.layernorm_before.bias	model.swin.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table	model.swin.encoder.layers.0.blocks.0.attention.self.query.weight	model.swin.encoder.layers.0.blocks.0.attention.self.query.bias	model.swin.encoder.layers.0.blocks.0.attention.self.key.weight	model.swin.encoder.layers.0.blocks.0.attention.self.key.bias	model.swin.encoder.layers.0.blocks.0.attention.self.value.weight	model.swin.encoder.layers.0.blocks.0.attention.self.value.bias	model.swin.encoder.layers.0.blocks.0.attention.output.dense.weight	model.swin.encoder.layers.0.blocks.0.attention.output.dense.bias	model.swin.encoder.layers.0.blocks.0.layernorm_after.weight	model.swin.encoder.layers.0.blocks.0.layernorm_after.bias	model.swin.encoder.layers.0.blocks.0.intermediate.dense.weight	model.swin.encoder.layers.0.blocks.0.intermediate.dense.bias	model.swin.encoder.layers.0.blocks.0.output.dense.weight	model.swin.encoder.layers.0.blocks.0.output.dense.bias	model.swin.encoder.layers.0.blocks.1.layernorm_before.weight	model.swin.encoder.layers.0.blocks.1.layernorm_before.bias	model.swin.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table	model.swin.encoder.layers.0.blocks.1.attention.self.query.weight	model.swin.encoder.layers.0.blocks.1.attention.self.query.bias	model.swin.encoder.layers.0.blocks.1.attention.self.key.weight	model.swin.encoder.layers.0.blocks.1.attention.self.key.bias	model.swin.encoder.layers.0.blocks.1.attention.self.value.weight	model.swin.encoder.layers.0.blocks.1.attention.self.value.bias	model.swin.encoder.layers.0.blocks.1.attention.output.dense.weight	model.swin.encoder.layers.0.blocks.1.attention.output.dense.bias	model.swin.encoder.layers.0.blocks.1.layernorm_after.weight	model.swin.encoder.layers.0.blocks.1.layernorm_after.bias	model.swin.encoder.layers.0.blocks.1.intermediate.dense.weight	model.swin.encoder.layers.0.blocks.1.intermediate.dense.bias	model.swin.encoder.layers.0.blocks.1.output.dense.weight	model.swin.encoder.layers.0.blocks.1.output.dense.bias	model.swin.encoder.layers.0.downsample.reduction.weight	model.swin.encoder.layers.0.downsample.norm.weight	model.swin.encoder.layers.0.downsample.norm.bias	model.swin.encoder.layers.1.blocks.0.layernorm_before.weight	model.swin.encoder.layers.1.blocks.0.layernorm_before.bias	model.swin.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table	model.swin.encoder.layers.1.blocks.0.attention.self.query.weight	model.swin.encoder.layers.1.blocks.0.attention.self.query.bias	model.swin.encoder.layers.1.blocks.0.attention.self.key.weight	model.swin.encoder.layers.1.blocks.0.attention.self.key.bias	model.swin.encoder.layers.1.blocks.0.attention.self.value.weight	model.swin.encoder.layers.1.blocks.0.attention.self.value.bias	model.swin.encoder.layers.1.blocks.0.attention.output.dense.weight	model.swin.encoder.layers.1.blocks.0.attention.output.dense.bias	model.swin.encoder.layers.1.blocks.0.layernorm_after.weight	model.swin.encoder.layers.1.blocks.0.layernorm_after.bias	model.swin.encoder.layers.1.blocks.0.intermediate.dense.weight	model.swin.encoder.layers.1.blocks.0.intermediate.dense.bias	model.swin.encoder.layers.1.blocks.0.output.dense.weight	model.swin.encoder.layers.1.blocks.0.output.dense.bias	model.swin.encoder.layers.1.blocks.1.layernorm_before.weight	model.swin.encoder.layers.1.blocks.1.layernorm_before.bias	model.swin.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table	model.swin.encoder.layers.1.blocks.1.attention.self.query.weight	model.swin.encoder.layers.1.blocks.1.attention.self.query.bias	model.swin.encoder.layers.1.blocks.1.attention.self.key.weight	model.swin.encoder.layers.1.blocks.1.attention.self.key.bias	model.swin.encoder.layers.1.blocks.1.attention.self.value.weight	model.swin.encoder.layers.1.blocks.1.attention.self.value.bias	model.swin.encoder.layers.1.blocks.1.attention.output.dense.weight	model.swin.encoder.layers.1.blocks.1.attention.output.dense.bias	model.swin.encoder.layers.1.blocks.1.layernorm_after.weight	model.swin.encoder.layers.1.blocks.1.layernorm_after.bias	model.swin.encoder.layers.1.blocks.1.intermediate.dense.weight	model.swin.encoder.layers.1.blocks.1.intermediate.dense.bias	model.swin.encoder.layers.1.blocks.1.output.dense.weight	model.swin.encoder.layers.1.blocks.1.output.dense.bias	model.swin.encoder.layers.1.downsample.reduction.weight	model.swin.encoder.layers.1.downsample.norm.weight	model.swin.encoder.layers.1.downsample.norm.bias	model.swin.encoder.layers.2.blocks.0.layernorm_before.weight	model.swin.encoder.layers.2.blocks.0.layernorm_before.bias	model.swin.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.0.attention.self.query.weight	model.swin.encoder.layers.2.blocks.0.attention.self.query.bias	model.swin.encoder.layers.2.blocks.0.attention.self.key.weight	model.swin.encoder.layers.2.blocks.0.attention.self.key.bias	model.swin.encoder.layers.2.blocks.0.attention.self.value.weight	model.swin.encoder.layers.2.blocks.0.attention.self.value.bias	model.swin.encoder.layers.2.blocks.0.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.0.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.0.layernorm_after.weight	model.swin.encoder.layers.2.blocks.0.layernorm_after.bias	model.swin.encoder.layers.2.blocks.0.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.0.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.0.output.dense.weight	model.swin.encoder.layers.2.blocks.0.output.dense.bias	model.swin.encoder.layers.2.blocks.1.layernorm_before.weight	model.swin.encoder.layers.2.blocks.1.layernorm_before.bias	model.swin.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.1.attention.self.query.weight	model.swin.encoder.layers.2.blocks.1.attention.self.query.bias	model.swin.encoder.layers.2.blocks.1.attention.self.key.weight	model.swin.encoder.layers.2.blocks.1.attention.self.key.bias	model.swin.encoder.layers.2.blocks.1.attention.self.value.weight	model.swin.encoder.layers.2.blocks.1.attention.self.value.bias	model.swin.encoder.layers.2.blocks.1.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.1.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.1.layernorm_after.weight	model.swin.encoder.layers.2.blocks.1.layernorm_after.bias	model.swin.encoder.layers.2.blocks.1.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.1.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.1.output.dense.weight	model.swin.encoder.layers.2.blocks.1.output.dense.bias	model.swin.encoder.layers.2.blocks.2.layernorm_before.weight	model.swin.encoder.layers.2.blocks.2.layernorm_before.bias	model.swin.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.2.attention.self.query.weight	model.swin.encoder.layers.2.blocks.2.attention.self.query.bias	model.swin.encoder.layers.2.blocks.2.attention.self.key.weight	model.swin.encoder.layers.2.blocks.2.attention.self.key.bias	model.swin.encoder.layers.2.blocks.2.attention.self.value.weight	model.swin.encoder.layers.2.blocks.2.attention.self.value.bias	model.swin.encoder.layers.2.blocks.2.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.2.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.2.layernorm_after.weight	model.swin.encoder.layers.2.blocks.2.layernorm_after.bias	model.swin.encoder.layers.2.blocks.2.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.2.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.2.output.dense.weight	model.swin.encoder.layers.2.blocks.2.output.dense.bias	model.swin.encoder.layers.2.blocks.3.layernorm_before.weight	model.swin.encoder.layers.2.blocks.3.layernorm_before.bias	model.swin.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.3.attention.self.query.weight	model.swin.encoder.layers.2.blocks.3.attention.self.query.bias	model.swin.encoder.layers.2.blocks.3.attention.self.key.weight	model.swin.encoder.layers.2.blocks.3.attention.self.key.bias	model.swin.encoder.layers.2.blocks.3.attention.self.value.weight	model.swin.encoder.layers.2.blocks.3.attention.self.value.bias	model.swin.encoder.layers.2.blocks.3.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.3.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.3.layernorm_after.weight	model.swin.encoder.layers.2.blocks.3.layernorm_after.bias	model.swin.encoder.layers.2.blocks.3.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.3.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.3.output.dense.weight	model.swin.encoder.layers.2.blocks.3.output.dense.bias	model.swin.encoder.layers.2.blocks.4.layernorm_before.weight	model.swin.encoder.layers.2.blocks.4.layernorm_before.bias	model.swin.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.4.attention.self.query.weight	model.swin.encoder.layers.2.blocks.4.attention.self.query.bias	model.swin.encoder.layers.2.blocks.4.attention.self.key.weight	model.swin.encoder.layers.2.blocks.4.attention.self.key.bias	model.swin.encoder.layers.2.blocks.4.attention.self.value.weight	model.swin.encoder.layers.2.blocks.4.attention.self.value.bias	model.swin.encoder.layers.2.blocks.4.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.4.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.4.layernorm_after.weight	model.swin.encoder.layers.2.blocks.4.layernorm_after.bias	model.swin.encoder.layers.2.blocks.4.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.4.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.4.output.dense.weight	model.swin.encoder.layers.2.blocks.4.output.dense.bias	model.swin.encoder.layers.2.blocks.5.layernorm_before.weight	model.swin.encoder.layers.2.blocks.5.layernorm_before.bias	model.swin.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.5.attention.self.query.weight	model.swin.encoder.layers.2.blocks.5.attention.self.query.bias	model.swin.encoder.layers.2.blocks.5.attention.self.key.weight	model.swin.encoder.layers.2.blocks.5.attention.self.key.bias	model.swin.encoder.layers.2.blocks.5.attention.self.value.weight	model.swin.encoder.layers.2.blocks.5.attention.self.value.bias	model.swin.encoder.layers.2.blocks.5.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.5.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.5.layernorm_after.weight	model.swin.encoder.layers.2.blocks.5.layernorm_after.bias	model.swin.encoder.layers.2.blocks.5.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.5.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.5.output.dense.weight	model.swin.encoder.layers.2.blocks.5.output.dense.bias	model.swin.encoder.layers.2.blocks.6.layernorm_before.weight	model.swin.encoder.layers.2.blocks.6.layernorm_before.bias	model.swin.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.6.attention.self.query.weight	model.swin.encoder.layers.2.blocks.6.attention.self.query.bias	model.swin.encoder.layers.2.blocks.6.attention.self.key.weight	model.swin.encoder.layers.2.blocks.6.attention.self.key.bias	model.swin.encoder.layers.2.blocks.6.attention.self.value.weight	model.swin.encoder.layers.2.blocks.6.attention.self.value.bias	model.swin.encoder.layers.2.blocks.6.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.6.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.6.layernorm_after.weight	model.swin.encoder.layers.2.blocks.6.layernorm_after.bias	model.swin.encoder.layers.2.blocks.6.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.6.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.6.output.dense.weight	model.swin.encoder.layers.2.blocks.6.output.dense.bias	model.swin.encoder.layers.2.blocks.7.layernorm_before.weight	model.swin.encoder.layers.2.blocks.7.layernorm_before.bias	model.swin.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.7.attention.self.query.weight	model.swin.encoder.layers.2.blocks.7.attention.self.query.bias	model.swin.encoder.layers.2.blocks.7.attention.self.key.weight	model.swin.encoder.layers.2.blocks.7.attention.self.key.bias	model.swin.encoder.layers.2.blocks.7.attention.self.value.weight	model.swin.encoder.layers.2.blocks.7.attention.self.value.bias	model.swin.encoder.layers.2.blocks.7.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.7.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.7.layernorm_after.weight	model.swin.encoder.layers.2.blocks.7.layernorm_after.bias	model.swin.encoder.layers.2.blocks.7.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.7.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.7.output.dense.weight	model.swin.encoder.layers.2.blocks.7.output.dense.bias	model.swin.encoder.layers.2.blocks.8.layernorm_before.weight	model.swin.encoder.layers.2.blocks.8.layernorm_before.bias	model.swin.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.8.attention.self.query.weight	model.swin.encoder.layers.2.blocks.8.attention.self.query.bias	model.swin.encoder.layers.2.blocks.8.attention.self.key.weight	model.swin.encoder.layers.2.blocks.8.attention.self.key.bias	model.swin.encoder.layers.2.blocks.8.attention.self.value.weight	model.swin.encoder.layers.2.blocks.8.attention.self.value.bias	model.swin.encoder.layers.2.blocks.8.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.8.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.8.layernorm_after.weight	model.swin.encoder.layers.2.blocks.8.layernorm_after.bias	model.swin.encoder.layers.2.blocks.8.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.8.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.8.output.dense.weight	model.swin.encoder.layers.2.blocks.8.output.dense.bias	model.swin.encoder.layers.2.blocks.9.layernorm_before.weight	model.swin.encoder.layers.2.blocks.9.layernorm_before.bias	model.swin.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.9.attention.self.query.weight	model.swin.encoder.layers.2.blocks.9.attention.self.query.bias	model.swin.encoder.layers.2.blocks.9.attention.self.key.weight	model.swin.encoder.layers.2.blocks.9.attention.self.key.bias	model.swin.encoder.layers.2.blocks.9.attention.self.value.weight	model.swin.encoder.layers.2.blocks.9.attention.self.value.bias	model.swin.encoder.layers.2.blocks.9.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.9.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.9.layernorm_after.weight	model.swin.encoder.layers.2.blocks.9.layernorm_after.bias	model.swin.encoder.layers.2.blocks.9.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.9.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.9.output.dense.weight	model.swin.encoder.layers.2.blocks.9.output.dense.bias	model.swin.encoder.layers.2.blocks.10.layernorm_before.weight	model.swin.encoder.layers.2.blocks.10.layernorm_before.bias	model.swin.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.10.attention.self.query.weight	model.swin.encoder.layers.2.blocks.10.attention.self.query.bias	model.swin.encoder.layers.2.blocks.10.attention.self.key.weight	model.swin.encoder.layers.2.blocks.10.attention.self.key.bias	model.swin.encoder.layers.2.blocks.10.attention.self.value.weight	model.swin.encoder.layers.2.blocks.10.attention.self.value.bias	model.swin.encoder.layers.2.blocks.10.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.10.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.10.layernorm_after.weight	model.swin.encoder.layers.2.blocks.10.layernorm_after.bias	model.swin.encoder.layers.2.blocks.10.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.10.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.10.output.dense.weight	model.swin.encoder.layers.2.blocks.10.output.dense.bias	model.swin.encoder.layers.2.blocks.11.layernorm_before.weight	model.swin.encoder.layers.2.blocks.11.layernorm_before.bias	model.swin.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.11.attention.self.query.weight	model.swin.encoder.layers.2.blocks.11.attention.self.query.bias	model.swin.encoder.layers.2.blocks.11.attention.self.key.weight	model.swin.encoder.layers.2.blocks.11.attention.self.key.bias	model.swin.encoder.layers.2.blocks.11.attention.self.value.weight	model.swin.encoder.layers.2.blocks.11.attention.self.value.bias	model.swin.encoder.layers.2.blocks.11.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.11.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.11.layernorm_after.weight	model.swin.encoder.layers.2.blocks.11.layernorm_after.bias	model.swin.encoder.layers.2.blocks.11.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.11.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.11.output.dense.weight	model.swin.encoder.layers.2.blocks.11.output.dense.bias	model.swin.encoder.layers.2.blocks.12.layernorm_before.weight	model.swin.encoder.layers.2.blocks.12.layernorm_before.bias	model.swin.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.12.attention.self.query.weight	model.swin.encoder.layers.2.blocks.12.attention.self.query.bias	model.swin.encoder.layers.2.blocks.12.attention.self.key.weight	model.swin.encoder.layers.2.blocks.12.attention.self.key.bias	model.swin.encoder.layers.2.blocks.12.attention.self.value.weight	model.swin.encoder.layers.2.blocks.12.attention.self.value.bias	model.swin.encoder.layers.2.blocks.12.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.12.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.12.layernorm_after.weight	model.swin.encoder.layers.2.blocks.12.layernorm_after.bias	model.swin.encoder.layers.2.blocks.12.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.12.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.12.output.dense.weight	model.swin.encoder.layers.2.blocks.12.output.dense.bias	model.swin.encoder.layers.2.blocks.13.layernorm_before.weight	model.swin.encoder.layers.2.blocks.13.layernorm_before.bias	model.swin.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.13.attention.self.query.weight	model.swin.encoder.layers.2.blocks.13.attention.self.query.bias	model.swin.encoder.layers.2.blocks.13.attention.self.key.weight	model.swin.encoder.layers.2.blocks.13.attention.self.key.bias	model.swin.encoder.layers.2.blocks.13.attention.self.value.weight	model.swin.encoder.layers.2.blocks.13.attention.self.value.bias	model.swin.encoder.layers.2.blocks.13.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.13.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.13.layernorm_after.weight	model.swin.encoder.layers.2.blocks.13.layernorm_after.bias	model.swin.encoder.layers.2.blocks.13.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.13.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.13.output.dense.weight	model.swin.encoder.layers.2.blocks.13.output.dense.bias	model.swin.encoder.layers.2.blocks.14.layernorm_before.weight	model.swin.encoder.layers.2.blocks.14.layernorm_before.bias	model.swin.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.14.attention.self.query.weight	model.swin.encoder.layers.2.blocks.14.attention.self.query.bias	model.swin.encoder.layers.2.blocks.14.attention.self.key.weight	model.swin.encoder.layers.2.blocks.14.attention.self.key.bias	model.swin.encoder.layers.2.blocks.14.attention.self.value.weight	model.swin.encoder.layers.2.blocks.14.attention.self.value.bias	model.swin.encoder.layers.2.blocks.14.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.14.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.14.layernorm_after.weight	model.swin.encoder.layers.2.blocks.14.layernorm_after.bias	model.swin.encoder.layers.2.blocks.14.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.14.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.14.output.dense.weight	model.swin.encoder.layers.2.blocks.14.output.dense.bias	model.swin.encoder.layers.2.blocks.15.layernorm_before.weight	model.swin.encoder.layers.2.blocks.15.layernorm_before.bias	model.swin.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.15.attention.self.query.weight	model.swin.encoder.layers.2.blocks.15.attention.self.query.bias	model.swin.encoder.layers.2.blocks.15.attention.self.key.weight	model.swin.encoder.layers.2.blocks.15.attention.self.key.bias	model.swin.encoder.layers.2.blocks.15.attention.self.value.weight	model.swin.encoder.layers.2.blocks.15.attention.self.value.bias	model.swin.encoder.layers.2.blocks.15.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.15.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.15.layernorm_after.weight	model.swin.encoder.layers.2.blocks.15.layernorm_after.bias	model.swin.encoder.layers.2.blocks.15.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.15.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.15.output.dense.weight	model.swin.encoder.layers.2.blocks.15.output.dense.bias	model.swin.encoder.layers.2.blocks.16.layernorm_before.weight	model.swin.encoder.layers.2.blocks.16.layernorm_before.bias	model.swin.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.16.attention.self.query.weight	model.swin.encoder.layers.2.blocks.16.attention.self.query.bias	model.swin.encoder.layers.2.blocks.16.attention.self.key.weight	model.swin.encoder.layers.2.blocks.16.attention.self.key.bias	model.swin.encoder.layers.2.blocks.16.attention.self.value.weight	model.swin.encoder.layers.2.blocks.16.attention.self.value.bias	model.swin.encoder.layers.2.blocks.16.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.16.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.16.layernorm_after.weight	model.swin.encoder.layers.2.blocks.16.layernorm_after.bias	model.swin.encoder.layers.2.blocks.16.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.16.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.16.output.dense.weight	model.swin.encoder.layers.2.blocks.16.output.dense.bias	model.swin.encoder.layers.2.blocks.17.layernorm_before.weight	model.swin.encoder.layers.2.blocks.17.layernorm_before.bias	model.swin.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table	model.swin.encoder.layers.2.blocks.17.attention.self.query.weight	model.swin.encoder.layers.2.blocks.17.attention.self.query.bias	model.swin.encoder.layers.2.blocks.17.attention.self.key.weight	model.swin.encoder.layers.2.blocks.17.attention.self.key.bias	model.swin.encoder.layers.2.blocks.17.attention.self.value.weight	model.swin.encoder.layers.2.blocks.17.attention.self.value.bias	model.swin.encoder.layers.2.blocks.17.attention.output.dense.weight	model.swin.encoder.layers.2.blocks.17.attention.output.dense.bias	model.swin.encoder.layers.2.blocks.17.layernorm_after.weight	model.swin.encoder.layers.2.blocks.17.layernorm_after.bias	model.swin.encoder.layers.2.blocks.17.intermediate.dense.weight	model.swin.encoder.layers.2.blocks.17.intermediate.dense.bias	model.swin.encoder.layers.2.blocks.17.output.dense.weight	model.swin.encoder.layers.2.blocks.17.output.dense.bias	model.swin.encoder.layers.2.downsample.reduction.weight	model.swin.encoder.layers.2.downsample.norm.weight	model.swin.encoder.layers.2.downsample.norm.bias	model.swin.encoder.layers.3.blocks.0.layernorm_before.weight	model.swin.encoder.layers.3.blocks.0.layernorm_before.bias	model.swin.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table	model.swin.encoder.layers.3.blocks.0.attention.self.query.weight	model.swin.encoder.layers.3.blocks.0.attention.self.query.bias	model.swin.encoder.layers.3.blocks.0.attention.self.key.weight	model.swin.encoder.layers.3.blocks.0.attention.self.key.bias	model.swin.encoder.layers.3.blocks.0.attention.self.value.weight	model.swin.encoder.layers.3.blocks.0.attention.self.value.bias	model.swin.encoder.layers.3.blocks.0.attention.output.dense.weight	model.swin.encoder.layers.3.blocks.0.attention.output.dense.bias	model.swin.encoder.layers.3.blocks.0.layernorm_after.weight	model.swin.encoder.layers.3.blocks.0.layernorm_after.bias	model.swin.encoder.layers.3.blocks.0.intermediate.dense.weight	model.swin.encoder.layers.3.blocks.0.intermediate.dense.bias	model.swin.encoder.layers.3.blocks.0.output.dense.weight	model.swin.encoder.layers.3.blocks.0.output.dense.bias	model.swin.encoder.layers.3.blocks.1.layernorm_before.weight	model.swin.encoder.layers.3.blocks.1.layernorm_before.bias	model.swin.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table	model.swin.encoder.layers.3.blocks.1.attention.self.query.weight	model.swin.encoder.layers.3.blocks.1.attention.self.query.bias	model.swin.encoder.layers.3.blocks.1.attention.self.key.weight	model.swin.encoder.layers.3.blocks.1.attention.self.key.bias	model.swin.encoder.layers.3.blocks.1.attention.self.value.weight	model.swin.encoder.layers.3.blocks.1.attention.self.value.bias	model.swin.encoder.layers.3.blocks.1.attention.output.dense.weight	model.swin.encoder.layers.3.blocks.1.attention.output.dense.bias	model.swin.encoder.layers.3.blocks.1.layernorm_after.weight	model.swin.encoder.layers.3.blocks.1.layernorm_after.bias	model.swin.encoder.layers.3.blocks.1.intermediate.dense.weight	model.swin.encoder.layers.3.blocks.1.intermediate.dense.bias	model.swin.encoder.layers.3.blocks.1.output.dense.weight	model.swin.encoder.layers.3.blocks.1.output.dense.bias	model.swin.layernorm.weight	model.swin.layernorm.bias	predictor.weight	predictor.bias
using cuda

------- Experiment Summary --------
id: 0
dataset: CIFAR100 	batch size: 32 	lr: 0.0001
num train batch: 1563 	num validation batch: 313 	num test batch: 313
finetune method: all
param count: 86845724 86845724
wrapper2D(
  (model): SwinForImageClassification(
    (swin): SwinModel(
      (embeddings): Embeddings2D(
        (resize): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)
        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (encoder): SwinEncoder(
        (layers): ModuleList(
          (0): SwinStage(
            (blocks): ModuleList(
              (0-1): 2 x SwinLayer(
                (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=128, out_features=128, bias=True)
                    (key): Linear(in_features=128, out_features=128, bias=True)
                    (value): Linear(in_features=128, out_features=128, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=128, out_features=128, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.1)
                (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=128, out_features=512, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=512, out_features=128, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (downsample): SwinPatchMerging(
              (reduction): Linear(in_features=512, out_features=256, bias=False)
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): SwinStage(
            (blocks): ModuleList(
              (0-1): 2 x SwinLayer(
                (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=256, out_features=256, bias=True)
                    (key): Linear(in_features=256, out_features=256, bias=True)
                    (value): Linear(in_features=256, out_features=256, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=256, out_features=256, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.1)
                (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=256, out_features=1024, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=1024, out_features=256, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (downsample): SwinPatchMerging(
              (reduction): Linear(in_features=1024, out_features=512, bias=False)
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): SwinStage(
            (blocks): ModuleList(
              (0-17): 18 x SwinLayer(
                (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=512, out_features=512, bias=True)
                    (key): Linear(in_features=512, out_features=512, bias=True)
                    (value): Linear(in_features=512, out_features=512, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=512, out_features=512, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.1)
                (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=512, out_features=2048, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=2048, out_features=512, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (downsample): SwinPatchMerging(
              (reduction): Linear(in_features=2048, out_features=1024, bias=False)
              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): SwinStage(
            (blocks): ModuleList(
              (0-1): 2 x SwinLayer(
                (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.1)
                (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=1024, out_features=4096, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=4096, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (pooler): AdaptiveAvgPool1d(output_size=1)
    )
    (classifier): Identity()
    (pooler): AdaptiveAvgPool1d(output_size=1)
  )
  (predictor): Linear(in_features=1024, out_features=100, bias=True)
  (embedder): Embeddings2D(
    (resize): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)
    (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
)

------- Start Training --------
epoch:  0
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[train full 0 0.000020 ] time elapsed: 445.4737 	train loss: 4.6192 	val loss: 4.5424 	val score: 0.9737 	best val score: 0.9737
epoch:  1
[train full 1 0.000040 ] time elapsed: 444.5016 	train loss: 4.3388 	val loss: 3.9756 	val score: 0.7876 	best val score: 0.7876
epoch:  2
[train full 2 0.000060 ] time elapsed: 444.9364 	train loss: 2.7935 	val loss: 1.1468 	val score: 0.2151 	best val score: 0.2151
epoch:  3
[train full 3 0.000080 ] time elapsed: 444.6080 	train loss: 0.8510 	val loss: 1.5931 	val score: 0.4146 	best val score: 0.2151
epoch:  4
[train full 4 0.000100 ] time elapsed: 444.4615 	train loss: 1.0533 	val loss: 0.5436 	val score: 0.1572 	best val score: 0.1572
epoch:  5
[train full 5 0.000099 ] time elapsed: 444.6243 	train loss: 0.6470 	val loss: 0.4863 	val score: 0.1430 	best val score: 0.1430
epoch:  6
[train full 6 0.000099 ] time elapsed: 444.6209 	train loss: 0.4962 	val loss: 0.3938 	val score: 0.1208 	best val score: 0.1208
epoch:  7
[train full 7 0.000098 ] time elapsed: 444.5483 	train loss: 0.4388 	val loss: 0.3674 	val score: 0.1133 	best val score: 0.1133
epoch:  8
[train full 8 0.000098 ] time elapsed: 444.8290 	train loss: 0.3842 	val loss: 0.3475 	val score: 0.1070 	best val score: 0.1070
epoch:  9
[train full 9 0.000097 ] time elapsed: 444.7113 	train loss: 0.3382 	val loss: 0.3416 	val score: 0.1054 	best val score: 0.1054
epoch:  10
[train full 10 0.000097 ] time elapsed: 444.6835 	train loss: 0.3242 	val loss: 0.3166 	val score: 0.0979 	best val score: 0.0979
epoch:  11
[train full 11 0.000096 ] time elapsed: 444.5452 	train loss: 0.2919 	val loss: 0.3011 	val score: 0.0916 	best val score: 0.0916
epoch:  12
[train full 12 0.000096 ] time elapsed: 444.7354 	train loss: 0.3090 	val loss: 0.2967 	val score: 0.0910 	best val score: 0.0910
epoch:  13
[train full 13 0.000095 ] time elapsed: 444.4226 	train loss: 0.2793 	val loss: 0.2886 	val score: 0.0895 	best val score: 0.0895
epoch:  14
[train full 14 0.000095 ] time elapsed: 444.3589 	train loss: 0.2780 	val loss: 0.2935 	val score: 0.0907 	best val score: 0.0895
epoch:  15
[train full 15 0.000094 ] time elapsed: 444.4755 	train loss: 0.2626 	val loss: 0.2831 	val score: 0.0875 	best val score: 0.0875
epoch:  16
[train full 16 0.000094 ] time elapsed: 444.7394 	train loss: 0.2341 	val loss: 0.2825 	val score: 0.0883 	best val score: 0.0875
epoch:  17
[train full 17 0.000093 ] time elapsed: 444.6249 	train loss: 0.2246 	val loss: 0.2777 	val score: 0.0861 	best val score: 0.0861
epoch:  18
[train full 18 0.000093 ] time elapsed: 444.5081 	train loss: 0.2477 	val loss: 0.2822 	val score: 0.0884 	best val score: 0.0861
epoch:  19
[train full 19 0.000092 ] time elapsed: 444.5073 	train loss: 0.2117 	val loss: 0.2687 	val score: 0.0832 	best val score: 0.0832
epoch:  20
[train full 20 0.000092 ] time elapsed: 444.7666 	train loss: 0.2004 	val loss: 0.2647 	val score: 0.0822 	best val score: 0.0822
epoch:  21
[train full 21 0.000091 ] time elapsed: 444.5470 	train loss: 0.1909 	val loss: 0.2680 	val score: 0.0830 	best val score: 0.0822
epoch:  22
[train full 22 0.000091 ] time elapsed: 444.5276 	train loss: 0.1926 	val loss: 0.2689 	val score: 0.0839 	best val score: 0.0822
epoch:  23
[train full 23 0.000090 ] time elapsed: 444.3468 	train loss: 0.1777 	val loss: 0.2630 	val score: 0.0810 	best val score: 0.0810
epoch:  24
[train full 24 0.000090 ] time elapsed: 444.6739 	train loss: 0.1736 	val loss: 0.2649 	val score: 0.0812 	best val score: 0.0810
epoch:  25
[train full 25 0.000089 ] time elapsed: 444.4050 	train loss: 0.1658 	val loss: 0.2591 	val score: 0.0801 	best val score: 0.0801
epoch:  26
[train full 26 0.000089 ] time elapsed: 444.5916 	train loss: 0.1723 	val loss: 0.2643 	val score: 0.0805 	best val score: 0.0801
epoch:  27
[train full 27 0.000088 ] time elapsed: 444.5724 	train loss: 0.1556 	val loss: 0.2606 	val score: 0.0782 	best val score: 0.0782
epoch:  28
[train full 28 0.000088 ] time elapsed: 444.9497 	train loss: 0.1508 	val loss: 0.2627 	val score: 0.0799 	best val score: 0.0782
epoch:  29
[train full 29 0.000087 ] time elapsed: 444.5526 	train loss: 0.1568 	val loss: 0.2653 	val score: 0.0809 	best val score: 0.0782
epoch:  30
[train full 30 0.000087 ] time elapsed: 444.8273 	train loss: 0.1408 	val loss: 0.2551 	val score: 0.0783 	best val score: 0.0782
epoch:  31
[train full 31 0.000086 ] time elapsed: 444.6687 	train loss: 0.1318 	val loss: 0.2603 	val score: 0.0775 	best val score: 0.0775
epoch:  32
[train full 32 0.000086 ] time elapsed: 444.6879 	train loss: 0.1313 	val loss: 0.2565 	val score: 0.0757 	best val score: 0.0757
epoch:  33
[train full 33 0.000085 ] time elapsed: 444.6456 	train loss: 0.1429 	val loss: 0.2646 	val score: 0.0786 	best val score: 0.0757
epoch:  34
[train full 34 0.000085 ] time elapsed: 444.5592 	train loss: 0.1247 	val loss: 0.2550 	val score: 0.0768 	best val score: 0.0757
epoch:  35
[train full 35 0.000084 ] time elapsed: 444.4380 	train loss: 0.1194 	val loss: 0.2597 	val score: 0.0772 	best val score: 0.0757
epoch:  36
[train full 36 0.000084 ] time elapsed: 444.4336 	train loss: 0.1253 	val loss: 0.2720 	val score: 0.0803 	best val score: 0.0757
epoch:  37
[train full 37 0.000083 ] time elapsed: 444.4572 	train loss: 0.1168 	val loss: 0.2600 	val score: 0.0775 	best val score: 0.0757
epoch:  38
[train full 38 0.000083 ] time elapsed: 444.4233 	train loss: 0.1696 	val loss: 0.2814 	val score: 0.0851 	best val score: 0.0757
epoch:  39
[train full 39 0.000082 ] time elapsed: 444.6669 	train loss: 0.1199 	val loss: 0.2673 	val score: 0.0785 	best val score: 0.0757
epoch:  40
[train full 40 0.000082 ] time elapsed: 444.5835 	train loss: 0.1125 	val loss: 0.2686 	val score: 0.0789 	best val score: 0.0757
epoch:  41
[train full 41 0.000081 ] time elapsed: 444.6728 	train loss: 0.1067 	val loss: 0.2629 	val score: 0.0765 	best val score: 0.0757
epoch:  42
[train full 42 0.000081 ] time elapsed: 444.7610 	train loss: 0.1029 	val loss: 0.2673 	val score: 0.0776 	best val score: 0.0757
epoch:  43
[train full 43 0.000080 ] time elapsed: 444.8385 	train loss: 0.1008 	val loss: 0.2687 	val score: 0.0787 	best val score: 0.0757
epoch:  44
[train full 44 0.000079 ] time elapsed: 444.8147 	train loss: 0.0977 	val loss: 0.2818 	val score: 0.0802 	best val score: 0.0757
epoch:  45
[train full 45 0.000079 ] time elapsed: 444.7808 	train loss: 0.1035 	val loss: 0.2679 	val score: 0.0765 	best val score: 0.0757
epoch:  46
[train full 46 0.000078 ] time elapsed: 444.8005 	train loss: 0.1069 	val loss: 0.2711 	val score: 0.0800 	best val score: 0.0757
epoch:  47
[train full 47 0.000078 ] time elapsed: 444.7927 	train loss: 0.1054 	val loss: 0.2668 	val score: 0.0771 	best val score: 0.0757
epoch:  48
[train full 48 0.000077 ] time elapsed: 444.7849 	train loss: 0.0957 	val loss: 0.2653 	val score: 0.0759 	best val score: 0.0757
epoch:  49
[train full 49 0.000077 ] time elapsed: 444.7074 	train loss: 0.0865 	val loss: 0.2696 	val score: 0.0771 	best val score: 0.0757
epoch:  50
[train full 50 0.000076 ] time elapsed: 444.7137 	train loss: 0.0816 	val loss: 0.2621 	val score: 0.0747 	best val score: 0.0747
epoch:  51
[train full 51 0.000076 ] time elapsed: 444.8799 	train loss: 0.0797 	val loss: 0.2618 	val score: 0.0758 	best val score: 0.0747
epoch:  52
[train full 52 0.000075 ] time elapsed: 444.4414 	train loss: 0.0768 	val loss: 0.2630 	val score: 0.0749 	best val score: 0.0747
epoch:  53
[train full 53 0.000075 ] time elapsed: 444.4926 	train loss: 0.0869 	val loss: 0.2701 	val score: 0.0788 	best val score: 0.0747
epoch:  54
[train full 54 0.000074 ] time elapsed: 444.6274 	train loss: 0.0799 	val loss: 0.2628 	val score: 0.0734 	best val score: 0.0734
epoch:  55
[train full 55 0.000074 ] time elapsed: 444.8274 	train loss: 0.0773 	val loss: 0.2638 	val score: 0.0742 	best val score: 0.0734
epoch:  56
[train full 56 0.000073 ] time elapsed: 444.4821 	train loss: 0.0730 	val loss: 0.2663 	val score: 0.0738 	best val score: 0.0734
epoch:  57
[train full 57 0.000073 ] time elapsed: 444.5352 	train loss: 0.0690 	val loss: 0.2628 	val score: 0.0734 	best val score: 0.0734
epoch:  58
[train full 58 0.000072 ] time elapsed: 444.7499 	train loss: 0.0647 	val loss: 0.2697 	val score: 0.0734 	best val score: 0.0734
epoch:  59
[train full 59 0.000072 ] time elapsed: 444.5408 	train loss: 0.0651 	val loss: 0.2650 	val score: 0.0734 	best val score: 0.0734

------- Start Test --------
[test last] 	time elapsed: 32.0869 	test loss: 0.2650 	test score: 0.0734
[test best-validated] 	time elapsed: 32.2280 	test loss: 0.2650 	test score: 0.0734

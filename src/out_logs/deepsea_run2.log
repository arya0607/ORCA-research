ot.gpu not found - coupling computation will be in cpu
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode
config:  ./configs/deepsea.yaml
args.device:  cuda
NVIDIA GeForce RTX 3090
NVIDIA GeForce RTX 3090
target_seq_len 512
/home/aryas/ORCA/src
./datasets
src feat shape torch.Size([2000, 768]) torch.Size([2000]) num classes 10
class weights
0 3630 74243 0.04889349837695136
1 35091 74243 0.4726506202604959
2 2458 74243 0.033107498350012796
3 3651 74243 0.049176353326239514
4 2137 74243 0.028783858410893957
5 4727 74243 0.06366930215643225
6 8048 74243 0.108400791993858
7 4821 74243 0.0649354147865792
8 3002 74243 0.04043478846490578
9 6678 74243 0.08994787387363118
Params to learn: 	embedder.norm.weight	embedder.norm.bias	embedder.position_embeddings.weight	embedder.projection.weight	embedder.projection.bias	predictor.weight	predictor.bias
[train embedder 0 0.000100 ] time elapsed: 7.4671 	otdd loss: 46.4086
[train embedder 1 0.000100 ] time elapsed: 5.0870 	otdd loss: 46.0222
[train embedder 2 0.000100 ] time elapsed: 4.6657 	otdd loss: 45.5734
[train embedder 3 0.000100 ] time elapsed: 6.1397 	otdd loss: 45.2499
[train embedder 4 0.000100 ] time elapsed: 4.4116 	otdd loss: 44.8822
[train embedder 5 0.000100 ] time elapsed: 4.6205 	otdd loss: 44.4744
[train embedder 6 0.000100 ] time elapsed: 5.0006 	otdd loss: 44.1480
[train embedder 7 0.000100 ] time elapsed: 4.4186 	otdd loss: 43.5309
[train embedder 8 0.000100 ] time elapsed: 4.3448 	otdd loss: 43.2763
[train embedder 9 0.000100 ] time elapsed: 4.4041 	otdd loss: 42.8800
[train embedder 10 0.000100 ] time elapsed: 4.8900 	otdd loss: 42.6476
[train embedder 11 0.000100 ] time elapsed: 4.3875 	otdd loss: 42.1739
[train embedder 12 0.000100 ] time elapsed: 4.8348 	otdd loss: 41.8253
[train embedder 13 0.000100 ] time elapsed: 4.4871 	otdd loss: 41.4276
[train embedder 14 0.000100 ] time elapsed: 6.3505 	otdd loss: 41.0322
[train embedder 15 0.000100 ] time elapsed: 5.0432 	otdd loss: 40.7223
[train embedder 16 0.000100 ] time elapsed: 6.0314 	otdd loss: 40.3127
[train embedder 17 0.000100 ] time elapsed: 4.3248 	otdd loss: 39.9699
[train embedder 18 0.000100 ] time elapsed: 6.9230 	otdd loss: 39.6132
[train embedder 19 0.000100 ] time elapsed: 4.8481 	otdd loss: 39.2846
[train embedder 20 0.000100 ] time elapsed: 5.2980 	otdd loss: 38.8269
[train embedder 21 0.000020 ] time elapsed: 6.2294 	otdd loss: 38.4890
[train embedder 22 0.000020 ] time elapsed: 5.3133 	otdd loss: 38.3781
[train embedder 23 0.000020 ] time elapsed: 7.1019 	otdd loss: 38.4282
[train embedder 24 0.000020 ] time elapsed: 5.5094 	otdd loss: 38.3521
[train embedder 25 0.000020 ] time elapsed: 4.4778 	otdd loss: 38.3449
[train embedder 26 0.000020 ] time elapsed: 5.0368 	otdd loss: 38.1419
[train embedder 27 0.000020 ] time elapsed: 4.8556 	otdd loss: 38.1924
[train embedder 28 0.000020 ] time elapsed: 5.9256 	otdd loss: 38.3189
[train embedder 29 0.000020 ] time elapsed: 7.2641 	otdd loss: 37.9846
[train embedder 30 0.000020 ] time elapsed: 5.0209 	otdd loss: 38.0616
[train embedder 31 0.000020 ] time elapsed: 5.2216 	otdd loss: 37.8856
[train embedder 32 0.000020 ] time elapsed: 4.3886 	otdd loss: 37.9132
[train embedder 33 0.000020 ] time elapsed: 4.8088 	otdd loss: 37.7785
[train embedder 34 0.000020 ] time elapsed: 4.3248 	otdd loss: 37.7266
[train embedder 35 0.000020 ] time elapsed: 5.5011 	otdd loss: 37.7191
[train embedder 36 0.000020 ] time elapsed: 4.4030 	otdd loss: 37.5775
[train embedder 37 0.000020 ] time elapsed: 5.7445 	otdd loss: 37.4494
[train embedder 38 0.000020 ] time elapsed: 5.9713 	otdd loss: 37.3686
[train embedder 39 0.000020 ] time elapsed: 4.5146 	otdd loss: 37.3436
[train embedder 40 0.000020 ] time elapsed: 4.9305 	otdd loss: 37.2477
[train embedder 41 0.000004 ] time elapsed: 5.9244 	otdd loss: 37.2506
[train embedder 42 0.000004 ] time elapsed: 5.1740 	otdd loss: 37.1329
[train embedder 43 0.000004 ] time elapsed: 4.9746 	otdd loss: 37.1594
[train embedder 44 0.000004 ] time elapsed: 4.3444 	otdd loss: 37.2078
[train embedder 45 0.000004 ] time elapsed: 4.5961 	otdd loss: 37.3077
[train embedder 46 0.000004 ] time elapsed: 5.1069 	otdd loss: 37.1649
[train embedder 47 0.000004 ] time elapsed: 7.0496 	otdd loss: 37.1102
[train embedder 48 0.000004 ] time elapsed: 7.0258 	otdd loss: 37.1439
[train embedder 49 0.000004 ] time elapsed: 4.5791 	otdd loss: 37.1486
[train embedder 50 0.000004 ] time elapsed: 5.6262 	otdd loss: 37.0835
[train embedder 51 0.000004 ] time elapsed: 4.3164 	otdd loss: 37.1380
[train embedder 52 0.000004 ] time elapsed: 6.1632 	otdd loss: 37.0061
[train embedder 53 0.000004 ] time elapsed: 4.3657 	otdd loss: 36.9968
[train embedder 54 0.000004 ] time elapsed: 5.1644 	otdd loss: 37.0540
[train embedder 55 0.000004 ] time elapsed: 4.5033 	otdd loss: 37.2178
[train embedder 56 0.000004 ] time elapsed: 4.4432 	otdd loss: 37.0634
[train embedder 57 0.000004 ] time elapsed: 4.5655 	otdd loss: 37.0560
[train embedder 58 0.000004 ] time elapsed: 4.7964 	otdd loss: 36.9738
[train embedder 59 0.000004 ] time elapsed: 4.5937 	otdd loss: 37.0810
[train embedder 60 0.000004 ] time elapsed: 4.3042 	otdd loss: 36.9345
[train embedder 61 0.000001 ] time elapsed: 4.6343 	otdd loss: 36.9013
[train embedder 62 0.000001 ] time elapsed: 4.3411 	otdd loss: 36.9204
Params to learn: 	model.encoder.layer.0.attention.self.query.weight	model.encoder.layer.0.attention.self.query.bias	model.encoder.layer.0.attention.self.key.weight	model.encoder.layer.0.attention.self.key.bias	model.encoder.layer.0.attention.self.value.weight	model.encoder.layer.0.attention.self.value.bias	model.encoder.layer.0.attention.output.dense.weight	model.encoder.layer.0.attention.output.dense.bias	model.encoder.layer.0.attention.output.LayerNorm.weight	model.encoder.layer.0.attention.output.LayerNorm.bias	model.encoder.layer.0.intermediate.dense.weight	model.encoder.layer.0.intermediate.dense.bias	model.encoder.layer.0.output.dense.weight	model.encoder.layer.0.output.dense.bias	model.encoder.layer.0.output.LayerNorm.weight	model.encoder.layer.0.output.LayerNorm.bias	model.encoder.layer.1.attention.self.query.weight	model.encoder.layer.1.attention.self.query.bias	model.encoder.layer.1.attention.self.key.weight	model.encoder.layer.1.attention.self.key.bias	model.encoder.layer.1.attention.self.value.weight	model.encoder.layer.1.attention.self.value.bias	model.encoder.layer.1.attention.output.dense.weight	model.encoder.layer.1.attention.output.dense.bias	model.encoder.layer.1.attention.output.LayerNorm.weight	model.encoder.layer.1.attention.output.LayerNorm.bias	model.encoder.layer.1.intermediate.dense.weight	model.encoder.layer.1.intermediate.dense.bias	model.encoder.layer.1.output.dense.weight	model.encoder.layer.1.output.dense.bias	model.encoder.layer.1.output.LayerNorm.weight	model.encoder.layer.1.output.LayerNorm.bias	model.encoder.layer.2.attention.self.query.weight	model.encoder.layer.2.attention.self.query.bias	model.encoder.layer.2.attention.self.key.weight	model.encoder.layer.2.attention.self.key.bias	model.encoder.layer.2.attention.self.value.weight	model.encoder.layer.2.attention.self.value.bias	model.encoder.layer.2.attention.output.dense.weight	model.encoder.layer.2.attention.output.dense.bias	model.encoder.layer.2.attention.output.LayerNorm.weight	model.encoder.layer.2.attention.output.LayerNorm.bias	model.encoder.layer.2.intermediate.dense.weight	model.encoder.layer.2.intermediate.dense.bias	model.encoder.layer.2.output.dense.weight	model.encoder.layer.2.output.dense.bias	model.encoder.layer.2.output.LayerNorm.weight	model.encoder.layer.2.output.LayerNorm.bias	model.encoder.layer.3.attention.self.query.weight	model.encoder.layer.3.attention.self.query.bias	model.encoder.layer.3.attention.self.key.weight	model.encoder.layer.3.attention.self.key.bias	model.encoder.layer.3.attention.self.value.weight	model.encoder.layer.3.attention.self.value.bias	model.encoder.layer.3.attention.output.dense.weight	model.encoder.layer.3.attention.output.dense.bias	model.encoder.layer.3.attention.output.LayerNorm.weight	model.encoder.layer.3.attention.output.LayerNorm.bias	model.encoder.layer.3.intermediate.dense.weight	model.encoder.layer.3.intermediate.dense.bias	model.encoder.layer.3.output.dense.weight	model.encoder.layer.3.output.dense.bias	model.encoder.layer.3.output.LayerNorm.weight	model.encoder.layer.3.output.LayerNorm.bias	model.encoder.layer.4.attention.self.query.weight	model.encoder.layer.4.attention.self.query.bias	model.encoder.layer.4.attention.self.key.weight	model.encoder.layer.4.attention.self.key.bias	model.encoder.layer.4.attention.self.value.weight	model.encoder.layer.4.attention.self.value.bias	model.encoder.layer.4.attention.output.dense.weight	model.encoder.layer.4.attention.output.dense.bias	model.encoder.layer.4.attention.output.LayerNorm.weight	model.encoder.layer.4.attention.output.LayerNorm.bias	model.encoder.layer.4.intermediate.dense.weight	model.encoder.layer.4.intermediate.dense.bias	model.encoder.layer.4.output.dense.weight	model.encoder.layer.4.output.dense.bias	model.encoder.layer.4.output.LayerNorm.weight	model.encoder.layer.4.output.LayerNorm.bias	model.encoder.layer.5.attention.self.query.weight	model.encoder.layer.5.attention.self.query.bias	model.encoder.layer.5.attention.self.key.weight	model.encoder.layer.5.attention.self.key.bias	model.encoder.layer.5.attention.self.value.weight	model.encoder.layer.5.attention.self.value.bias	model.encoder.layer.5.attention.output.dense.weight	model.encoder.layer.5.attention.output.dense.bias	model.encoder.layer.5.attention.output.LayerNorm.weight	model.encoder.layer.5.attention.output.LayerNorm.bias	model.encoder.layer.5.intermediate.dense.weight	model.encoder.layer.5.intermediate.dense.bias	model.encoder.layer.5.output.dense.weight	model.encoder.layer.5.output.dense.bias	model.encoder.layer.5.output.LayerNorm.weight	model.encoder.layer.5.output.LayerNorm.bias	model.encoder.layer.6.attention.self.query.weight	model.encoder.layer.6.attention.self.query.bias	model.encoder.layer.6.attention.self.key.weight	model.encoder.layer.6.attention.self.key.bias	model.encoder.layer.6.attention.self.value.weight	model.encoder.layer.6.attention.self.value.bias	model.encoder.layer.6.attention.output.dense.weight	model.encoder.layer.6.attention.output.dense.bias	model.encoder.layer.6.attention.output.LayerNorm.weight	model.encoder.layer.6.attention.output.LayerNorm.bias	model.encoder.layer.6.intermediate.dense.weight	model.encoder.layer.6.intermediate.dense.bias	model.encoder.layer.6.output.dense.weight	model.encoder.layer.6.output.dense.bias	model.encoder.layer.6.output.LayerNorm.weight	model.encoder.layer.6.output.LayerNorm.bias	model.encoder.layer.7.attention.self.query.weight	model.encoder.layer.7.attention.self.query.bias	model.encoder.layer.7.attention.self.key.weight	model.encoder.layer.7.attention.self.key.bias	model.encoder.layer.7.attention.self.value.weight	model.encoder.layer.7.attention.self.value.bias	model.encoder.layer.7.attention.output.dense.weight	model.encoder.layer.7.attention.output.dense.bias	model.encoder.layer.7.attention.output.LayerNorm.weight	model.encoder.layer.7.attention.output.LayerNorm.bias	model.encoder.layer.7.intermediate.dense.weight	model.encoder.layer.7.intermediate.dense.bias	model.encoder.layer.7.output.dense.weight	model.encoder.layer.7.output.dense.bias	model.encoder.layer.7.output.LayerNorm.weight	model.encoder.layer.7.output.LayerNorm.bias	model.encoder.layer.8.attention.self.query.weight	model.encoder.layer.8.attention.self.query.bias	model.encoder.layer.8.attention.self.key.weight	model.encoder.layer.8.attention.self.key.bias	model.encoder.layer.8.attention.self.value.weight	model.encoder.layer.8.attention.self.value.bias	model.encoder.layer.8.attention.output.dense.weight	model.encoder.layer.8.attention.output.dense.bias	model.encoder.layer.8.attention.output.LayerNorm.weight	model.encoder.layer.8.attention.output.LayerNorm.bias	model.encoder.layer.8.intermediate.dense.weight	model.encoder.layer.8.intermediate.dense.bias	model.encoder.layer.8.output.dense.weight	model.encoder.layer.8.output.dense.bias	model.encoder.layer.8.output.LayerNorm.weight	model.encoder.layer.8.output.LayerNorm.bias	model.encoder.layer.9.attention.self.query.weight	model.encoder.layer.9.attention.self.query.bias	model.encoder.layer.9.attention.self.key.weight	model.encoder.layer.9.attention.self.key.bias	model.encoder.layer.9.attention.self.value.weight	model.encoder.layer.9.attention.self.value.bias	model.encoder.layer.9.attention.output.dense.weight	model.encoder.layer.9.attention.output.dense.bias	model.encoder.layer.9.attention.output.LayerNorm.weight	model.encoder.layer.9.attention.output.LayerNorm.bias	model.encoder.layer.9.intermediate.dense.weight	model.encoder.layer.9.intermediate.dense.bias	model.encoder.layer.9.output.dense.weight	model.encoder.layer.9.output.dense.bias	model.encoder.layer.9.output.LayerNorm.weight	model.encoder.layer.9.output.LayerNorm.bias	model.encoder.layer.10.attention.self.query.weight	model.encoder.layer.10.attention.self.query.bias	model.encoder.layer.10.attention.self.key.weight	model.encoder.layer.10.attention.self.key.bias	model.encoder.layer.10.attention.self.value.weight	model.encoder.layer.10.attention.self.value.bias	model.encoder.layer.10.attention.output.dense.weight	model.encoder.layer.10.attention.output.dense.bias	model.encoder.layer.10.attention.output.LayerNorm.weight	model.encoder.layer.10.attention.output.LayerNorm.bias	model.encoder.layer.10.intermediate.dense.weight	model.encoder.layer.10.intermediate.dense.bias	model.encoder.layer.10.output.dense.weight	model.encoder.layer.10.output.dense.bias	model.encoder.layer.10.output.LayerNorm.weight	model.encoder.layer.10.output.LayerNorm.bias	model.encoder.layer.11.attention.self.query.weight	model.encoder.layer.11.attention.self.query.bias	model.encoder.layer.11.attention.self.key.weight	model.encoder.layer.11.attention.self.key.bias	model.encoder.layer.11.attention.self.value.weight	model.encoder.layer.11.attention.self.value.bias	model.encoder.layer.11.attention.output.dense.weight	model.encoder.layer.11.attention.output.dense.bias	model.encoder.layer.11.attention.output.LayerNorm.weight	model.encoder.layer.11.attention.output.LayerNorm.bias	model.encoder.layer.11.intermediate.dense.weight	model.encoder.layer.11.intermediate.dense.bias	model.encoder.layer.11.output.dense.weight	model.encoder.layer.11.output.dense.bias	model.encoder.layer.11.output.LayerNorm.weight	model.encoder.layer.11.output.LayerNorm.bias	embedder.norm.weight	embedder.norm.bias	embedder.position_embeddings.weight	embedder.projection.weight	embedder.projection.bias	predictor.weight	predictor.bias
using cuda

------- Experiment Summary --------
id: 2
dataset: DEEPSEA 	batch size: 16 	lr: 1e-05
num train batch: 4641 	num validation batch: 9313 	num test batch: 9313
finetune method: all
param count: 85485348 85485348
wrapper1D(
  (model): RobertaModel(
    (embeddings): embedder_placeholder()
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0, inplace=False)
            )
          )
          (intermediate): RbertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_acto_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (pooler): adaptive_pooler(
      (pooler): AdaptiveAvgPool1d(output_size=1)
    )
  )
  (embedder): Embeddings1D(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (projection): Conv1d(4, 768, kernel_size=(2,), stride=(2,))
  )
  (predictor): Linear(in_features=768, out_features=36, bias=True)
)

------- Start Training --------
epoch:  0
[train full 0 0.000001 ] time elapsed: 1558.1400 	train loss: 0.8549 	val loss: 0.8211 	val score: 0.3750 	best val score: 0.3750
epoch:  1
[train full 1 0.000002 ] time elapsed: 1560.0768 	train loss: 0.8295 	val loss: 0.8160 	val score: 0.3607 	best val score: 0.3607
epoch:  2
[train full 2 0.000003 ] time elapsed: 1557.2192 	train loss: 0.8222 	val loss: 0.8244 	val score: 0.3570 	best val score: 0.3570
epoch:  3
[train full 3 0.000004 ] time elapsed: 1558.8835 	train loss: 0.8186 	val loss: 0.8076 	val score: 0.3451 	best val score: 0.3451
epoch:  4
[train full 4 0.000005 ] time elapsed: 1559.4131 	train loss: 0.8166 	val loss: 0.8048 	val score: 0.3406 	best val score: 0.3406
epoch:  5
[train full 5 0.000006 ] time elapsed: 1557.3995 	train loss: 0.8150 	val loss: 0.8070 	val score: 0.3400 	best val score: 0.3400
epoch:  6
[train full 6 0.000007 ] time elapsed: 1559.4307 	train loss: 0.8141 	val loss: 0.8057 	val score: 0.3428 	best val score: 0.3400
epoch:  7
[train full 7 0.000008 ] time elapsed: 1559.4369 	train loss: 0.8122 	val loss: 0.8029 	val score: 0.3394 	best val score: 0.3394
epoch:  8
[train full 8 0.000009 ] time elapsed: 1559.4936 	train loss: 0.8106 	val loss: 0.8069 	val score: 0.3395 	best val score: 0.3394
epoch:  9
[train full 9 0.000010 ] time elapsed: 1557.9730 	train loss: 0.8091 	val loss: 0.8069 	val score: 0.3413 	best val score: 0.3394
epoch:  10
[train full 10 0.000009 ] time elapsed: 1559.6132 	train loss: 0.8053 	val loss: 0.8066 	val score: 0.3364 	best val score: 0.3364
epoch:  11
[train full 11 0.000009 ] time elapsed: 1560.4453 	train loss: 0.7985 	val loss: 0.8051 	val score: 0.3385 	best val score: 0.3364
epoch:  12
[train full 12 0.000008 ] time elapsed: 1559.2803 	train loss: 0.7898 	val loss: 0.8059 	val score: 0.3400 	best val score: 0.3364
epoch:  13
[train full 13 0.000007 ] time elapsed: 1558.5062 	train loss: 0.7753 	val loss: 0.8152 	val score: 0.3410 	best val score: 0.3364

------- Start Test --------
[test last] 	time elapsed: 1113.8706 	test loss: 0.8152 	test score: 0.3410
[test best-validated] 	time elapsed: 1113.7776 	test loss: 0.8066 	test score: 0.3365

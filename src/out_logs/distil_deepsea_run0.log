ot.gpu not found - coupling computation will be in cpu
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/home/aryas/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode
config:  ./configs/deepsea.yaml
args.device:  cuda
NVIDIA GeForce RTX 3090
target_seq_len 512
/home/aryas/ORCA/src
./datasets
src feat shape torch.Size([2000, 768]) torch.Size([2000]) num classes 10
class weights
0 26815 74243 0.3611788316743666
1 2813 74243 0.03788909392131245
2 6083 74243 0.08193365031046698
3 5032 74243 0.06777743356276013
4 5264 74243 0.07090230728822919
5 4835 74243 0.0651239847527713
6 7193 74243 0.09688455477284054
7 6271 74243 0.08446587557076088
8 5281 74243 0.0711312851043196
9 4656 74243 0.06271298304217232
Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 480/480 [00:00<00:00, 153kB/s]
Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]Downloading model.safetensors:   3%|▎         | 10.5M/331M [00:00<00:06, 46.1MB/s]Downloading model.safetensors:   6%|▋         | 21.0M/331M [00:00<00:05, 61.0MB/s]Downloading model.safetensors:  10%|▉         | 31.5M/331M [00:00<00:04, 70.3MB/s]Downloading model.safetensors:  13%|█▎        | 41.9M/331M [00:00<00:03, 78.7MB/s]Downloading model.safetensors:  16%|█▌        | 52.4M/331M [00:00<00:03, 78.2MB/s]Downloading model.safetensors:  19%|█▉        | 62.9M/331M [00:00<00:03, 85.3MB/s]Downloading model.safetensors:  22%|██▏       | 73.4M/331M [00:00<00:03, 83.3MB/s]Downloading model.safetensors:  25%|██▌       | 83.9M/331M [00:01<00:02, 84.9MB/s]Downloading model.safetensors:  29%|██▊       | 94.4M/331M [00:01<00:02, 85.0MB/s]Downloading model.safetensors:  32%|███▏      | 105M/331M [00:01<00:02, 89.5MB/s] Downloading model.safetensors:  35%|███▍      | 115M/331M [00:01<00:02, 86.5MB/s]Downloading model.safetensors:  38%|███▊      | 126M/331M [00:01<00:02, 87.0MB/s]Downloading model.safetensors:  44%|████▍     | 147M/331M [00:01<00:01, 93.0MB/s]Downloading model.safetensors:  51%|█████     | 168M/331M [00:01<00:01, 93.4MB/s]Downloading model.safetensors:  54%|█████▍    | 178M/331M [00:02<00:01, 90.9MB/s]Downloading model.safetensors:  57%|█████▋    | 189M/331M [00:02<00:01, 90.6MB/s]Downloading model.safetensors:  60%|██████    | 199M/331M [00:02<00:01, 92.9MB/s]Downloading model.safetensors:  63%|██████▎   | 210M/331M [00:02<00:01, 93.7MB/s]Downloading model.safetensors:  67%|██████▋   | 220M/331M [00:02<00:01, 91.1MB/s]Downloading model.safetensors:  70%|██████▉   | 231M/331M [00:02<00:01, 93.9MB/s]Downloading model.safetensors:  76%|███████▌  | 252M/331M [00:02<00:00, 94.2MB/s]Downloading model.safetensors:  79%|███████▉  | 262M/331M [00:03<00:00, 90.5MB/s]Downloading model.safetensors:  82%|████████▏ | 273M/331M [00:03<00:00, 85.7MB/s]Downloading model.safetensors:  86%|████████▌ | 283M/331M [00:03<00:00, 88.5MB/s]Downloading model.safetensors:  89%|████████▊ | 294M/331M [00:03<00:00, 85.6MB/s]Downloading model.safetensors:  92%|█████████▏| 304M/331M [00:03<00:00, 81.7MB/s]Downloading model.safetensors:  95%|█████████▌| 315M/331M [00:03<00:00, 81.1MB/s]Downloading model.safetensors:  98%|█████████▊| 325M/331M [00:03<00:00, 81.3MB/s]Downloading model.safetensors: 100%|██████████| 331M/331M [00:03<00:00, 85.6MB/s]
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Params to learn: 	embedder.norm.weight	embedder.norm.bias	embedder.position_embeddings.weight	embedder.projection.weight	embedder.projection.bias	predictor.weight	predictor.bias
[train embedder 0 0.000100 ] time elapsed: 5.5035 	otdd loss: 45.5326
[train embedder 1 0.000100 ] time elapsed: 3.6818 	otdd loss: 45.1170
[train embedder 2 0.000100 ] time elapsed: 3.6755 	otdd loss: 44.7046
[train embedder 3 0.000100 ] time elapsed: 3.6506 	otdd loss: 44.3662
[train embedder 4 0.000100 ] time elapsed: 3.7281 	otdd loss: 43.9687
[train embedder 5 0.000100 ] time elapsed: 3.8274 	otdd loss: 43.5315
[train embedder 6 0.000100 ] time elapsed: 3.7224 	otdd loss: 43.1781
[train embedder 7 0.000100 ] time elapsed: 3.7066 	otdd loss: 42.9013
[train embedder 8 0.000100 ] time elapsed: 3.6644 	otdd loss: 42.4081
[train embedder 9 0.000100 ] time elapsed: 3.6927 	otdd loss: 42.0897
[train embedder 10 0.000100 ] time elapsed: 3.7002 	otdd loss: 41.7052
[train embedder 11 0.000100 ] time elapsed: 3.6727 	otdd loss: 41.2709
[train embedder 12 0.000100 ] time elapsed: 3.6745 	otdd loss: 40.9512
[train embedder 13 0.000100 ] time elapsed: 3.7670 	otdd loss: 40.5246
[train embedder 14 0.000100 ] time elapsed: 3.7322 	otdd loss: 40.2714
[train embedder 15 0.000100 ] time elapsed: 3.7114 	otdd loss: 39.8144
[train embedder 16 0.000100 ] time elapsed: 3.6913 	otdd loss: 39.5248
[train embedder 17 0.000100 ] time elapsed: 3.7568 	otdd loss: 39.2532
[train embedder 18 0.000100 ] time elapsed: 3.8485 	otdd loss: 38.7528
[train embedder 19 0.000100 ] time elapsed: 3.7347 	otdd loss: 38.4954
[train embedder 20 0.000100 ] time elapsed: 3.6775 	otdd loss: 38.1862
[train embedder 21 0.000020 ] time elapsed: 3.6805 	otdd loss: 37.7453
[train embedder 22 0.000020 ] time elapsed: 3.7517 	otdd loss: 37.7712
[train embedder 23 0.000020 ] time elapsed: 3.7094 	otdd loss: 37.6217
[train embedder 24 0.000020 ] time elapsed: 3.7571 	otdd loss: 37.5426
[train embedder 25 0.000020 ] time elapsed: 3.7245 	otdd loss: 37.4519
[train embedder 26 0.000020 ] time elapsed: 3.7607 	otdd loss: 37.4126
[train embedder 27 0.000020 ] time elapsed: 3.7365 	otdd loss: 37.4394
[train embedder 28 0.000020 ] time elapsed: 3.7501 	otdd loss: 37.3258
[train embedder 29 0.000020 ] time elapsed: 3.6631 	otdd loss: 37.2559
[train embedder 30 0.000020 ] time elapsed: 3.8268 	otdd loss: 37.0841
[train embedder 31 0.000020 ] time elapsed: 3.6836 	otdd loss: 37.0319
[train embedder 32 0.000020 ] time elapsed: 3.6703 	otdd loss: 36.9706
[train embedder 33 0.000020 ] time elapsed: 3.6771 	otdd loss: 37.0499
[train embedder 34 0.000020 ] time elapsed: 3.6812 	otdd loss: 36.8522
[train embedder 35 0.000020 ] time elapsed: 3.6486 	otdd loss: 36.8733
[train embedder 36 0.000020 ] time elapsed: 3.6811 	otdd loss: 36.6971
[train embedder 37 0.000020 ] time elapsed: 3.6595 	otdd loss: 36.6962
[train embedder 38 0.000020 ] time elapsed: 3.7231 	otdd loss: 36.6482
[train embedder 39 0.000020 ] time elapsed: 3.7597 	otdd loss: 36.4830
[train embedder 40 0.000020 ] time elapsed: 3.7337 	otdd loss: 36.5628
[train embedder 41 0.000004 ] time elapsed: 3.7285 	otdd loss: 36.3926
[train embedder 42 0.000004 ] time elapsed: 3.7543 	otdd loss: 36.4220
[train embedder 43 0.000004 ] time elapsed: 3.8763 	otdd loss: 36.4220
[train embedder 44 0.000004 ] time elapsed: 3.7452 	otdd loss: 36.3947
[train embedder 45 0.000004 ] time elapsed: 3.7758 	otdd loss: 36.4707
[train embedder 46 0.000004 ] time elapsed: 3.7728 	otdd loss: 36.3731
[train embedder 47 0.000004 ] time elapsed: 3.7786 	otdd loss: 36.3930
[train embedder 48 0.000004 ] time elapsed: 3.7866 	otdd loss: 36.2405
[train embedder 49 0.000004 ] time elapsed: 3.7286 	otdd loss: 36.3106
[train embedder 50 0.000004 ] time elapsed: 3.7538 	otdd loss: 36.2975
[train embedder 51 0.000004 ] time elapsed: 3.7410 	otdd loss: 36.3604
[train embedder 52 0.000004 ] time elapsed: 3.7137 	otdd loss: 36.3144
[train embedder 53 0.000004 ] time elapsed: 3.7358 	otdd loss: 36.2285
[train embedder 54 0.000004 ] time elapsed: 3.8872 	otdd loss: 36.3252
[train embedder 55 0.000004 ] time elapsed: 3.7653 	otdd loss: 36.2831
[train embedder 56 0.000004 ] time elapsed: 3.7664 	otdd loss: 36.2333
[train embedder 57 0.000004 ] time elapsed: 3.7557 	otdd loss: 36.2889
[train embedder 58 0.000004 ] time elapsed: 3.8058 	otdd loss: 36.2482
[train embedder 59 0.000004 ] time elapsed: 3.7664 	otdd loss: 36.2352
embedder_stats_saved:  None
Params to learn: 	model.encoder.layer.0.attention.self.query.weight	model.encoder.layer.0.attention.self.query.bias	model.encoder.layer.0.attention.self.key.weight	model.encoder.layer.0.attention.self.key.bias	model.encoder.layer.0.attention.self.value.weight	model.encoder.layer.0.attention.self.value.bias	model.encoder.layer.0.attention.output.dense.weight	model.encoder.layer.0.attention.output.dense.bias	model.encoder.layer.0.attention.output.LayerNorm.weight	model.encoder.layer.0.attention.output.LayerNorm.bias	model.encoder.layer.0.intermediate.dense.weight	model.encoder.layer.0.intermediate.dense.bias	model.encoder.layer.0.output.dense.weight	model.encoder.layer.0.output.dense.bias	model.encoder.layer.0.output.LayerNorm.weight	model.encoder.layer.0.output.LayerNorm.bias	model.encoder.layer.1.attention.self.query.weight	model.encoder.layer.1.attention.self.query.bias	model.encoder.layer.1.attention.self.key.weight	model.encoder.layer.1.attention.self.key.bias	model.encoder.layer.1.attention.self.value.weight	model.encoder.layer.1.attention.self.value.bias	model.encoder.layer.1.attention.output.dense.weight	model.encoder.layer.1.attention.output.dense.bias	model.encoder.layer.1.attention.output.LayerNorm.weight	model.encoder.layer.1.attention.output.LayerNorm.bias	model.encoder.layer.1.intermediate.dense.weight	model.encoder.layer.1.intermediate.dense.bias	model.encoder.layer.1.output.dense.weight	model.encoder.layer.1.output.dense.bias	model.encoder.layer.1.output.LayerNorm.weight	model.encoder.layer.1.output.LayerNorm.bias	model.encoder.layer.2.attention.self.query.weight	model.encoder.layer.2.attention.self.query.bias	model.encoder.layer.2.attention.self.key.weight	model.encoder.layer.2.attention.self.key.bias	model.encoder.layer.2.attention.self.value.weight	model.encoder.layer.2.attention.self.value.bias	model.encoder.layer.2.attention.output.dense.weight	model.encoder.layer.2.attention.output.dense.bias	model.encoder.layer.2.attention.output.LayerNorm.weight	model.encoder.layer.2.attention.output.LayerNorm.bias	model.encoder.layer.2.intermediate.dense.weight	model.encoder.layer.2.intermediate.dense.bias	model.encoder.layer.2.output.dense.weight	model.encoder.layer.2.output.dense.bias	model.encoder.layer.2.output.LayerNorm.weight	model.encoder.layer.2.output.LayerNorm.bias	model.encoder.layer.3.attention.self.query.weight	model.encoder.layer.3.attention.self.query.bias	model.encoder.layer.3.attention.self.key.weight	model.encoder.layer.3.attention.self.key.bias	model.encoder.layer.3.attention.self.value.weight	model.encoder.layer.3.attention.self.value.bias	model.encoder.layer.3.attention.output.dense.weight	model.encoder.layer.3.attention.output.dense.bias	model.encoder.layer.3.attention.output.LayerNorm.weight	model.encoder.layer.3.attention.output.LayerNorm.bias	model.encoder.layer.3.intermediate.dense.weight	model.encoder.layer.3.intermediate.dense.bias	model.encoder.layer.3.output.dense.weight	model.encoder.layer.3.output.dense.bias	model.encoder.layer.3.output.LayerNorm.weight	model.encoder.layer.3.output.LayerNorm.bias	model.encoder.layer.4.attention.self.query.weight	model.encoder.layer.4.attention.self.query.bias	model.encoder.layer.4.attention.self.key.weight	model.encoder.layer.4.attention.self.key.bias	model.encoder.layer.4.attention.self.value.weight	model.encoder.layer.4.attention.self.value.bias	model.encoder.layer.4.attention.output.dense.weight	model.encoder.layer.4.attention.output.dense.bias	model.encoder.layer.4.attention.output.LayerNorm.weight	model.encoder.layer.4.attention.output.LayerNorm.bias	model.encoder.layer.4.intermediate.dense.weight	model.encoder.layer.4.intermediate.dense.bias	model.encoder.layer.4.output.dense.weight	model.encoder.layer.4.output.dense.bias	model.encoder.layer.4.output.LayerNorm.weight	model.encoder.layer.4.output.LayerNorm.bias	model.encoder.layer.5.attention.self.query.weight	model.encoder.layer.5.attention.self.query.bias	model.encoder.layer.5.attention.self.key.weight	model.encoder.layer.5.attention.self.key.bias	model.encoder.layer.5.attention.self.value.weight	model.encoder.layer.5.attention.self.value.bias	model.encoder.layer.5.attention.output.dense.weight	model.encoder.layer.5.attention.output.dense.bias	model.encoder.layer.5.attention.output.LayerNorm.weight	model.encoder.layer.5.attention.output.LayerNorm.bias	model.encoder.layer.5.intermediate.dense.weight	model.encoder.layer.5.intermediate.dense.bias	model.encoder.layer.5.output.dense.weight	model.encoder.layer.5.output.dense.bias	model.encoder.layer.5.output.LayerNorm.weight	model.encoder.layer.5.output.LayerNorm.bias	embedder.norm.weight	embedder.norm.bias	embedder.position_embeddings.weight	embedder.projection.weight	embedder.projection.bias	predictor.weight	predictor.bias
using cuda

------- Experiment Summary --------
id: 10
dataset: DEEPSEA 	batch size: 16 	lr: 1e-05
num train batch: 4641 	num validation batch: 9313 	num test batch: 9313
finetune method: all
param count: 42958116 42958116
wrapper1D(
  (model): RobertaModel(
    (embeddings): embedder_placeholder()
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-5): 6 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (pooler): adaptive_pooler(
      (pooler): AdaptiveAvgPool1d(output_size=1)
    )
  )
  (embedder): Embeddings1D(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (projection): Conv1d(4, 768, kernel_size=(2,), stride=(2,))
  )
  (predictor): Linear(in_features=768, out_features=36, bias=True)
)

------- Start Training --------
epoch:  0
[train full 0 0.000001 ] time elapsed: 793.0654 	train loss: 0.8679 	val loss: 0.8226 	val score: 0.3784 	best val score: 0.3784
epoch:  1
[train full 1 0.000002 ] time elapsed: 790.5954 	train loss: 0.8294 	val loss: 0.8184 	val score: 0.3642 	best val score: 0.3642
epoch:  2
[train full 2 0.000003 ] time elapsed: 791.4273 	train loss: 0.8220 	val loss: 0.8079 	val score: 0.3490 	best val score: 0.3490
epoch:  3
[train full 3 0.000004 ] time elapsed: 791.1861 	train loss: 0.8189 	val loss: 0.8093 	val score: 0.3480 	best val score: 0.3480
epoch:  4
[train full 4 0.000005 ] time elapsed: 791.8261 	train loss: 0.8174 	val loss: 0.8122 	val score: 0.3473 	best val score: 0.3473
epoch:  5
[train full 5 0.000006 ] time elapsed: 792.2989 	train loss: 0.8153 	val loss: 0.8089 	val score: 0.3450 	best val score: 0.3450
epoch:  6
[train full 6 0.000007 ] time elapsed: 792.6224 	train loss: 0.8142 	val loss: 0.8055 	val score: 0.3402 	best val score: 0.3402
epoch:  7
[train full 7 0.000008 ] time elapsed: 792.4582 	train loss: 0.8125 	val loss: 0.8057 	val score: 0.3395 	best val score: 0.3395
epoch:  8
[train full 8 0.000009 ] time elapsed: 791.8786 	train loss: 0.8109 	val loss: 0.8082 	val score: 0.3403 	best val score: 0.3395
epoch:  9
[train full 9 0.000010 ] time elapsed: 792.0197 	train loss: 0.8098 	val loss: 0.8058 	val score: 0.3391 	best val score: 0.3391
epoch:  10
[train full 10 0.000009 ] time elapsed: 792.4073 	train loss: 0.8070 	val loss: 0.8098 	val score: 0.3400 	best val score: 0.3391
epoch:  11
[train full 11 0.000009 ] time elapsed: 790.5724 	train loss: 0.8015 	val loss: 0.8006 	val score: 0.3319 	best val score: 0.3319
epoch:  12
[train full 12 0.000008 ] time elapsed: 792.1684 	train loss: 0.7937 	val loss: 0.8022 	val score: 0.3252 	best val score: 0.3252

------- Start Test --------
[test last] 	time elapsed: 568.0004 	test loss: 0.8022 	test score: 0.3252
[test best-validated] 	time elapsed: 567.9627 	test loss: 0.8022 	test score: 0.3252
